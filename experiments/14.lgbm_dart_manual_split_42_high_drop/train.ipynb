{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd2ca34-3410-4078-aa48-7adbba211ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import joblib\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from collections import Counter\n",
    "from itertools import repeat\n",
    "from lightgbm import LGBMClassifier, log_evaluation\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a084bef6-4434-40fb-bafc-d7d3e4c57a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.common import (\n",
    "    sigmoid, pad_column_name\n",
    ")\n",
    "from utils.constants import *\n",
    "from utils.eval_helpers import (\n",
    "    plot_roc_curves, plot_feature_importance, \n",
    "    amex_metric, get_final_metric_df, amex_metric_np, lgb_amex_metric,\n",
    "    TreeExperiment\n",
    ")\n",
    "from utils.eda_helpers import (\n",
    "    plot_missing_proportion_barchart, \n",
    "    get_cols\n",
    ")\n",
    "from utils.extraction_helpers import read_file\n",
    "from utils.feature_group import (\n",
    "    CATEGORY_COLUMNS, CONTINUOUS_COLUMNS, NON_FEATURE_COLUMNS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "67194602-57fc-4e92-9801-8d600e52a5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885beac-e245-43b6-a2cc-73e9f871d132",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d6d46155-ea8e-4212-8c63-5b7ac8b08d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (458913, 5064)\n",
      "CPU times: user 17.1 s, sys: 28 s, total: 45.1 s\n",
      "Wall time: 41.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_agg = read_file(f\"../{PROCESSED_DATA_PATH}/v6/train_agg.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a9c7632d-3cf9-4ff9-9467-9ebe9d2daecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (458913, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = read_file(f\"../{RAW_DATA_PATH}/train_labels.csv\")\n",
    "target = labels[\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "46942b4a-fd5a-4bd0-9dbd-7ceb4b99f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# train_agg = train_agg.drop(columns=NON_FEATURE_COLUMNS + [\"target\"], errors=\"ignore\")\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "ffe8f01a-341d-4cbe-8c46-8ae026830c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D_92_first', 'D_68_first', 'B_30_last', 'D_120_second_last', 'D_64_first', 'B_38_first', 'B_38_last', 'D_114_last', 'D_63_first', 'B_38_second_last', 'D_126_last', 'D_117_second_last', 'D_126_second_last', 'D_64_second_last', 'D_92_third_last', 'D_64_last', 'D_114_third_last', 'B_30_second_last', 'D_116_first', 'D_120_last', 'D_114_first', 'D_117_last', 'B_30_third_last', 'D_126_third_last', 'D_63_last', 'D_68_second_last', 'D_116_last', 'D_63_third_last', 'D_68_last', 'D_92_last', 'D_120_first', 'D_114_second_last', 'D_68_third_last', 'B_30_first', 'D_117_third_last', 'D_116_third_last', 'D_117_first', 'D_63_second_last', 'D_92_second_last', 'D_120_third_last', 'D_116_second_last', 'B_38_third_last', 'D_64_third_last', 'D_126_first']\n"
     ]
    }
   ],
   "source": [
    "cat_columns = get_cols(train_agg, CATEGORY_COLUMNS)\n",
    "cat_features = list(set(cat_columns).intersection(train_agg.columns))\n",
    "print(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b167a731-6108-48d0-85a6-42604a690325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_agg = pd.concat([train_agg, labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "80925934-1a9e-4dda-b8dc-4557eb8c6d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((458913, 5064), (458913,))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_agg.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a9220a-b8e0-4e8b-a3a3-cbe8e82470e6",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "23e515a9-789e-4f6c-bea4-a3a093aa8b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.23 s, sys: 1.98 s, total: 5.21 s\n",
      "Wall time: 1.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lgbm_dart = TreeExperiment(\n",
    "    exp_full_path=\"../../experiments/12.lgbm_dart_manual_split_42\",\n",
    "    seed=42, \n",
    "    model_path=\"dart_models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f9562b28-dc94-4a6e-ab64-4f2d02ef7e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = lgbm_dart.feature_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3a4e610a-7e00-4d78-a92e-4aeb42090ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = fi.loc[fi.iloc[:, 1:6].max(axis=1) < 22][\"feature\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "301dd9da-1456-4a0b-9275-07beec7fa2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = fi.loc[~fi[\"feature\"].isin(to_drop)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8a2b1e76-1c73-40ff-83db-1ceb66fe28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fi.to_csv(\"./previous_feature_importance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7087f58d-a7c2-4bb3-8365-bcdcada1f9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1056"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "3cf6cdfc-dad0-49cc-8959-458075bfb784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458913, 5064)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6161818d-2842-4c11-be3d-4ba2341b5fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 351 ms, sys: 2.01 s, total: 2.36 s\n",
      "Wall time: 3.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_agg = train_agg.drop(columns=to_drop, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d0dd0a24-a871-4456-a96c-a4e0d6a2afd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458913, 4008)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4bc40919-d29e-4132-97d6-5bb15bda6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_sprint_noob_cols = fi.loc[fi[\"agg_type\"] == \"previous_sprint\"].nsmallest(60, \"average_importance\")[\"feature\"].tolist()\n",
    "third_last_noob_cols = fi.loc[fi[\"agg_type\"] == \"third_last\"].nsmallest(90, \"average_importance\")[\"feature\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d6948fae-241d-48c9-9de6-34b8815958aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 358 ms, sys: 2.12 s, total: 2.48 s\n",
      "Wall time: 3.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_agg = train_agg.drop(columns=prev_sprint_noob_cols + third_last_noob_cols, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a63d04f5-bd77-4509-861c-bb5f0e133b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458913, 3858)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "26c8347c-828b-4ecb-b273-fa435f3b0d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(train_agg.columns) - set(fi[\"feature\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd096c6-31da-4440-a378-05f3a8b69b41",
   "metadata": {},
   "source": [
    "#### Find Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ec4a4f97-25b8-407d-9711-57db7dc74ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D_68_first', 'B_30_last', 'D_120_second_last', 'D_64_first', 'B_38_first', 'B_38_last', 'D_114_last', 'B_38_second_last', 'D_126_last', 'D_117_second_last', 'D_126_second_last', 'D_64_second_last', 'D_92_third_last', 'D_64_last', 'B_30_second_last', 'D_116_first', 'D_120_last', 'D_114_first', 'D_117_last', 'D_63_last', 'D_68_second_last', 'D_116_last', 'D_68_last', 'D_120_first', 'D_114_second_last', 'B_30_first', 'D_117_third_last', 'D_116_third_last', 'D_117_first', 'D_63_second_last', 'D_116_second_last', 'D_126_first']\n"
     ]
    }
   ],
   "source": [
    "cat_columns = get_cols(train_agg, CATEGORY_COLUMNS)\n",
    "cat_features = list(set(cat_columns).intersection(train_agg.columns))\n",
    "print(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a225972a-717d-4475-9694-4b312e62aa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(dict(cat_min=train_agg.loc[:, cat_features].min(), \n",
    "#                   cat_null=train_agg.loc[:, cat_features].isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e7d3f731-99f6-4f7f-b5a4-1360020d3bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 46.02it/s]\n"
     ]
    }
   ],
   "source": [
    "for cat_feature in tqdm(cat_features):\n",
    "    if train_agg[cat_feature].isnull().sum() > 0:\n",
    "        train_agg[cat_feature] = train_agg[cat_feature].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5c661ee3-a89e-49c5-bf1b-45cb0a3c4d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agg[cat_features] = (train_agg[cat_features] + 1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a2a81-1e54-47cc-8193-c34571555192",
   "metadata": {},
   "source": [
    "### REAL Stratify Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b3ab433a-22a2-41a0-9115-b63d1b13222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.14 s, sys: 3.18 s, total: 4.32 s\n",
      "Wall time: 6.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "normal_train_agg = train_agg.loc[labels[\"target\"] == 0]\n",
    "default_train_agg = train_agg.loc[labels[\"target\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ee120ab7-cc4d-4eb2-bd2a-4987a29f9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_predict_group_df = pd.read_csv(f\"normal_predict_group.csv\").drop(columns=\"target\")\n",
    "default_predict_group_df = pd.read_csv(f\"default_predict_group.csv\").drop(columns=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8c357c02-6cb5-48bb-aee0-86eb64543932",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_train_agg = normal_train_agg.merge(normal_predict_group_df, on=\"customer_ID\", how=\"left\")\n",
    "default_train_agg = default_train_agg.merge(default_predict_group_df, on=\"customer_ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "157d6345-4a81-4a99-8477-ad28b9a9d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal_train_agg = normal_train_agg.drop(columns=get_cols(normal_train_agg, \"target\"))\n",
    "# default_train_agg = default_train_agg.drop(columns=get_cols(default_train_agg, \"target\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "39e45a16-b553-4c19-b70e-906e9edd543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f1ed9673-b449-4ccd-be0e-906327ee52ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f3ad0490-f83a-4e2c-a1ac-676db012b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_indices = {}\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(normal_train_agg, normal_train_agg[\"group\"])):\n",
    "    normal_indices[fold] = (trn_ind, val_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "06a8b6b2-0e67-4e37-9b61-b67b6bd29b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_indices = {}\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(default_train_agg, default_train_agg[\"group\"])):\n",
    "    default_indices[fold] = (trn_ind, val_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7ed6506d-c26c-4c13-8e4c-8bdfd9e89695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold_indices = {}\n",
    "# for fold in range(5):\n",
    "#     a = normal_train_agg.loc[normal_indices[fold][1], \"customer_ID\"].tolist()\n",
    "#     b = default_train_agg.loc[default_indices[fold][1], \"customer_ID\"].tolist()\n",
    "#     kfold_indices[fold] = a + b\n",
    "# joblib.dump(kfold_indices, \"./5fold_val_cid.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce39854-1382-4213-a78b-99519a4512e3",
   "metadata": {},
   "source": [
    "### Hyperparams Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "bcbcfe03-5c23-4997-960c-82ce41cf72d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'first_metric_only': True,\n",
    "    'metric': \"binary_logloss\",\n",
    "    'boosting': 'dart',\n",
    "    'device': \"cpu\",\n",
    "    'seed': seed,\n",
    "    'num_leaves': 95,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.19,\n",
    "    'bagging_freq': 10,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l2': 10,\n",
    "    'min_data_in_leaf': 90,\n",
    "    'scale_pos_weight': 1.38,\n",
    "    'max_bins': 255,\n",
    "    'feature_fraction_bynode': 0.9,\n",
    "    'drop_rate': 0.095,\n",
    "    'skip_drop': 0.52\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "29b170e8-eac9-4d09-970d-14244f215eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [10500] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "5ca0e6e4-58f2-473d-9be4-4d8cfa0c4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ = train_.drop(columns=[\"customer_ID\", \"group\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "b100e374-aeb9-4507-b66a-541d1e7b40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_x_columns = ['customer_ID', 'target', 'group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "d7517c14-6b8b-455e-8cc1-3a0d53beb3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimator: 10500\n",
      "X_train: 367130\n",
      "X_val: 91783\n",
      "Y_train: 367130\n",
      "Y_validation: 91783\n",
      "Start Training fold 0\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.209369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 529303\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 3831\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.291038\ttraining's amex: 0.777948\tvalid_1's binary_logloss: 0.29667\tvalid_1's amex: 0.763353\n",
      "[1000]\ttraining's binary_logloss: 0.244996\ttraining's amex: 0.792334\tvalid_1's binary_logloss: 0.254078\tvalid_1's amex: 0.773342\n",
      "[1500]\ttraining's binary_logloss: 0.223039\ttraining's amex: 0.804124\tvalid_1's binary_logloss: 0.236139\tvalid_1's amex: 0.779351\n",
      "[2000]\ttraining's binary_logloss: 0.210919\ttraining's amex: 0.817364\tvalid_1's binary_logloss: 0.228656\tvalid_1's amex: 0.784178\n",
      "[2500]\ttraining's binary_logloss: 0.200819\ttraining's amex: 0.83072\tvalid_1's binary_logloss: 0.224267\tvalid_1's amex: 0.787376\n",
      "[3000]\ttraining's binary_logloss: 0.194707\ttraining's amex: 0.840582\tvalid_1's binary_logloss: 0.222621\tvalid_1's amex: 0.789934\n",
      "[3500]\ttraining's binary_logloss: 0.189055\ttraining's amex: 0.849469\tvalid_1's binary_logloss: 0.221514\tvalid_1's amex: 0.791121\n",
      "[4000]\ttraining's binary_logloss: 0.1836\ttraining's amex: 0.857968\tvalid_1's binary_logloss: 0.220532\tvalid_1's amex: 0.791363\n",
      "[4500]\ttraining's binary_logloss: 0.178098\ttraining's amex: 0.867043\tvalid_1's binary_logloss: 0.219714\tvalid_1's amex: 0.793139\n",
      "[5000]\ttraining's binary_logloss: 0.17286\ttraining's amex: 0.876201\tvalid_1's binary_logloss: 0.219207\tvalid_1's amex: 0.793505\n",
      "[5500]\ttraining's binary_logloss: 0.168176\ttraining's amex: 0.884204\tvalid_1's binary_logloss: 0.218728\tvalid_1's amex: 0.793823\n",
      "[6000]\ttraining's binary_logloss: 0.164675\ttraining's amex: 0.89061\tvalid_1's binary_logloss: 0.218587\tvalid_1's amex: 0.793582\n",
      "[6500]\ttraining's binary_logloss: 0.160126\ttraining's amex: 0.897945\tvalid_1's binary_logloss: 0.218217\tvalid_1's amex: 0.794099\n",
      "[7000]\ttraining's binary_logloss: 0.155732\ttraining's amex: 0.904983\tvalid_1's binary_logloss: 0.217972\tvalid_1's amex: 0.794205\n",
      "[7500]\ttraining's binary_logloss: 0.151758\ttraining's amex: 0.912099\tvalid_1's binary_logloss: 0.217743\tvalid_1's amex: 0.794774\n",
      "[8000]\ttraining's binary_logloss: 0.148078\ttraining's amex: 0.91886\tvalid_1's binary_logloss: 0.217606\tvalid_1's amex: 0.794404\n",
      "[8500]\ttraining's binary_logloss: 0.144102\ttraining's amex: 0.924939\tvalid_1's binary_logloss: 0.217476\tvalid_1's amex: 0.794746\n",
      "[9000]\ttraining's binary_logloss: 0.140532\ttraining's amex: 0.930828\tvalid_1's binary_logloss: 0.217248\tvalid_1's amex: 0.794017\n",
      "[9500]\ttraining's binary_logloss: 0.136568\ttraining's amex: 0.937504\tvalid_1's binary_logloss: 0.217151\tvalid_1's amex: 0.795047\n",
      "[10000]\ttraining's binary_logloss: 0.132953\ttraining's amex: 0.94255\tvalid_1's binary_logloss: 0.217044\tvalid_1's amex: 0.794712\n",
      "[10500]\ttraining's binary_logloss: 0.129333\ttraining's amex: 0.94849\tvalid_1's binary_logloss: 0.216963\tvalid_1's amex: 0.795099\n",
      "Our fold 0 CV score is 0.7948602488201664\n",
      "Number of estimator: 10500\n",
      "X_train: 367130\n",
      "X_val: 91783\n",
      "Y_train: 367130\n",
      "Y_validation: 91783\n",
      "Start Training fold 1\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.021038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 529342\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 3830\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.291427\ttraining's amex: 0.777316\tvalid_1's binary_logloss: 0.295779\tvalid_1's amex: 0.766199\n",
      "[1000]\ttraining's binary_logloss: 0.245605\ttraining's amex: 0.791012\tvalid_1's binary_logloss: 0.252973\tvalid_1's amex: 0.775467\n",
      "[1500]\ttraining's binary_logloss: 0.223567\ttraining's amex: 0.804282\tvalid_1's binary_logloss: 0.234605\tvalid_1's amex: 0.782425\n",
      "[2000]\ttraining's binary_logloss: 0.21154\ttraining's amex: 0.81614\tvalid_1's binary_logloss: 0.227218\tvalid_1's amex: 0.78802\n",
      "[2500]\ttraining's binary_logloss: 0.201403\ttraining's amex: 0.828692\tvalid_1's binary_logloss: 0.22284\tvalid_1's amex: 0.791884\n",
      "[3000]\ttraining's binary_logloss: 0.1953\ttraining's amex: 0.839022\tvalid_1's binary_logloss: 0.221247\tvalid_1's amex: 0.79393\n",
      "[3500]\ttraining's binary_logloss: 0.189563\ttraining's amex: 0.847733\tvalid_1's binary_logloss: 0.2201\tvalid_1's amex: 0.794545\n",
      "[4000]\ttraining's binary_logloss: 0.184102\ttraining's amex: 0.856638\tvalid_1's binary_logloss: 0.21911\tvalid_1's amex: 0.794806\n",
      "[4500]\ttraining's binary_logloss: 0.178583\ttraining's amex: 0.865357\tvalid_1's binary_logloss: 0.21827\tvalid_1's amex: 0.79616\n",
      "[5000]\ttraining's binary_logloss: 0.173313\ttraining's amex: 0.87357\tvalid_1's binary_logloss: 0.217708\tvalid_1's amex: 0.796533\n",
      "[5500]\ttraining's binary_logloss: 0.168624\ttraining's amex: 0.882585\tvalid_1's binary_logloss: 0.217234\tvalid_1's amex: 0.797578\n",
      "[6000]\ttraining's binary_logloss: 0.165087\ttraining's amex: 0.889259\tvalid_1's binary_logloss: 0.217071\tvalid_1's amex: 0.798514\n",
      "[6500]\ttraining's binary_logloss: 0.160521\ttraining's amex: 0.896983\tvalid_1's binary_logloss: 0.216702\tvalid_1's amex: 0.79858\n",
      "[7000]\ttraining's binary_logloss: 0.156127\ttraining's amex: 0.904224\tvalid_1's binary_logloss: 0.216425\tvalid_1's amex: 0.799587\n",
      "[7500]\ttraining's binary_logloss: 0.15215\ttraining's amex: 0.911347\tvalid_1's binary_logloss: 0.216162\tvalid_1's amex: 0.799286\n",
      "[8000]\ttraining's binary_logloss: 0.148422\ttraining's amex: 0.918119\tvalid_1's binary_logloss: 0.215967\tvalid_1's amex: 0.799864\n",
      "[8500]\ttraining's binary_logloss: 0.14446\ttraining's amex: 0.924315\tvalid_1's binary_logloss: 0.215801\tvalid_1's amex: 0.799727\n",
      "[9000]\ttraining's binary_logloss: 0.1409\ttraining's amex: 0.930693\tvalid_1's binary_logloss: 0.215733\tvalid_1's amex: 0.799156\n",
      "[9500]\ttraining's binary_logloss: 0.13691\ttraining's amex: 0.93682\tvalid_1's binary_logloss: 0.215546\tvalid_1's amex: 0.799955\n",
      "[10000]\ttraining's binary_logloss: 0.133296\ttraining's amex: 0.94249\tvalid_1's binary_logloss: 0.215429\tvalid_1's amex: 0.799617\n",
      "[10500]\ttraining's binary_logloss: 0.129696\ttraining's amex: 0.947738\tvalid_1's binary_logloss: 0.215306\tvalid_1's amex: 0.799847\n",
      "Our fold 1 CV score is 0.7996290737488683\n",
      "Number of estimator: 10500\n",
      "X_train: 367130\n",
      "X_val: 91783\n",
      "Y_train: 367130\n",
      "Y_validation: 91783\n",
      "Start Training fold 2\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.150229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 529327\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 3835\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.291482\ttraining's amex: 0.777265\tvalid_1's binary_logloss: 0.295944\tvalid_1's amex: 0.768708\n",
      "[1000]\ttraining's binary_logloss: 0.245523\ttraining's amex: 0.792082\tvalid_1's binary_logloss: 0.253199\tvalid_1's amex: 0.776731\n",
      "[1500]\ttraining's binary_logloss: 0.223545\ttraining's amex: 0.80455\tvalid_1's binary_logloss: 0.234887\tvalid_1's amex: 0.781788\n",
      "[2000]\ttraining's binary_logloss: 0.211435\ttraining's amex: 0.816193\tvalid_1's binary_logloss: 0.227246\tvalid_1's amex: 0.788495\n",
      "[2500]\ttraining's binary_logloss: 0.201218\ttraining's amex: 0.828673\tvalid_1's binary_logloss: 0.222635\tvalid_1's amex: 0.791817\n",
      "[3000]\ttraining's binary_logloss: 0.19514\ttraining's amex: 0.839227\tvalid_1's binary_logloss: 0.221156\tvalid_1's amex: 0.79397\n",
      "[3500]\ttraining's binary_logloss: 0.189412\ttraining's amex: 0.84827\tvalid_1's binary_logloss: 0.220068\tvalid_1's amex: 0.795416\n",
      "[4000]\ttraining's binary_logloss: 0.183969\ttraining's amex: 0.857259\tvalid_1's binary_logloss: 0.219095\tvalid_1's amex: 0.796872\n",
      "[4500]\ttraining's binary_logloss: 0.178482\ttraining's amex: 0.865573\tvalid_1's binary_logloss: 0.218296\tvalid_1's amex: 0.795933\n",
      "[5000]\ttraining's binary_logloss: 0.17327\ttraining's amex: 0.874229\tvalid_1's binary_logloss: 0.21776\tvalid_1's amex: 0.796684\n",
      "[5500]\ttraining's binary_logloss: 0.168591\ttraining's amex: 0.882336\tvalid_1's binary_logloss: 0.217334\tvalid_1's amex: 0.797087\n",
      "[6000]\ttraining's binary_logloss: 0.165094\ttraining's amex: 0.889157\tvalid_1's binary_logloss: 0.21719\tvalid_1's amex: 0.797832\n",
      "[6500]\ttraining's binary_logloss: 0.160538\ttraining's amex: 0.896399\tvalid_1's binary_logloss: 0.216866\tvalid_1's amex: 0.798006\n",
      "[7000]\ttraining's binary_logloss: 0.156153\ttraining's amex: 0.903518\tvalid_1's binary_logloss: 0.216665\tvalid_1's amex: 0.79869\n",
      "[7500]\ttraining's binary_logloss: 0.15214\ttraining's amex: 0.910878\tvalid_1's binary_logloss: 0.216467\tvalid_1's amex: 0.798773\n",
      "[8000]\ttraining's binary_logloss: 0.148389\ttraining's amex: 0.91732\tvalid_1's binary_logloss: 0.216275\tvalid_1's amex: 0.798966\n",
      "[8500]\ttraining's binary_logloss: 0.144368\ttraining's amex: 0.924148\tvalid_1's binary_logloss: 0.216066\tvalid_1's amex: 0.799742\n",
      "[9000]\ttraining's binary_logloss: 0.140839\ttraining's amex: 0.930306\tvalid_1's binary_logloss: 0.215904\tvalid_1's amex: 0.798938\n",
      "[9500]\ttraining's binary_logloss: 0.136903\ttraining's amex: 0.936504\tvalid_1's binary_logloss: 0.21585\tvalid_1's amex: 0.79921\n",
      "[10000]\ttraining's binary_logloss: 0.133249\ttraining's amex: 0.942126\tvalid_1's binary_logloss: 0.21574\tvalid_1's amex: 0.798696\n",
      "[10500]\ttraining's binary_logloss: 0.129621\ttraining's amex: 0.947724\tvalid_1's binary_logloss: 0.215654\tvalid_1's amex: 0.798763\n",
      "Our fold 2 CV score is 0.7985243400739916\n",
      "Number of estimator: 10500\n",
      "X_train: 367131\n",
      "X_val: 91782\n",
      "Y_train: 367131\n",
      "Y_validation: 91782\n",
      "Start Training fold 3\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.142414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 529342\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 3833\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.2915\ttraining's amex: 0.777463\tvalid_1's binary_logloss: 0.295199\tvalid_1's amex: 0.768295\n",
      "[1000]\ttraining's binary_logloss: 0.245575\ttraining's amex: 0.791271\tvalid_1's binary_logloss: 0.252488\tvalid_1's amex: 0.778012\n",
      "[1500]\ttraining's binary_logloss: 0.223645\ttraining's amex: 0.803698\tvalid_1's binary_logloss: 0.234303\tvalid_1's amex: 0.785219\n",
      "[2000]\ttraining's binary_logloss: 0.211569\ttraining's amex: 0.816673\tvalid_1's binary_logloss: 0.226656\tvalid_1's amex: 0.790254\n",
      "[2500]\ttraining's binary_logloss: 0.201508\ttraining's amex: 0.828959\tvalid_1's binary_logloss: 0.22205\tvalid_1's amex: 0.794127\n",
      "[3000]\ttraining's binary_logloss: 0.195332\ttraining's amex: 0.838763\tvalid_1's binary_logloss: 0.220382\tvalid_1's amex: 0.795242\n",
      "[3500]\ttraining's binary_logloss: 0.189641\ttraining's amex: 0.84785\tvalid_1's binary_logloss: 0.219244\tvalid_1's amex: 0.795759\n",
      "[4000]\ttraining's binary_logloss: 0.184138\ttraining's amex: 0.857272\tvalid_1's binary_logloss: 0.21824\tvalid_1's amex: 0.796153\n",
      "[4500]\ttraining's binary_logloss: 0.178598\ttraining's amex: 0.865499\tvalid_1's binary_logloss: 0.217416\tvalid_1's amex: 0.796629\n",
      "[5000]\ttraining's binary_logloss: 0.173383\ttraining's amex: 0.87422\tvalid_1's binary_logloss: 0.216937\tvalid_1's amex: 0.797689\n",
      "[5500]\ttraining's binary_logloss: 0.168681\ttraining's amex: 0.882079\tvalid_1's binary_logloss: 0.216487\tvalid_1's amex: 0.798154\n",
      "[6000]\ttraining's binary_logloss: 0.165167\ttraining's amex: 0.889468\tvalid_1's binary_logloss: 0.216331\tvalid_1's amex: 0.798581\n",
      "[6500]\ttraining's binary_logloss: 0.160577\ttraining's amex: 0.896627\tvalid_1's binary_logloss: 0.215909\tvalid_1's amex: 0.798611\n",
      "[7000]\ttraining's binary_logloss: 0.156176\ttraining's amex: 0.904198\tvalid_1's binary_logloss: 0.215764\tvalid_1's amex: 0.798452\n",
      "[7500]\ttraining's binary_logloss: 0.152225\ttraining's amex: 0.911138\tvalid_1's binary_logloss: 0.215579\tvalid_1's amex: 0.798642\n",
      "[8000]\ttraining's binary_logloss: 0.148543\ttraining's amex: 0.917452\tvalid_1's binary_logloss: 0.215454\tvalid_1's amex: 0.79941\n",
      "[8500]\ttraining's binary_logloss: 0.144547\ttraining's amex: 0.923697\tvalid_1's binary_logloss: 0.215356\tvalid_1's amex: 0.799198\n",
      "[9000]\ttraining's binary_logloss: 0.140971\ttraining's amex: 0.929736\tvalid_1's binary_logloss: 0.215238\tvalid_1's amex: 0.799495\n",
      "[9500]\ttraining's binary_logloss: 0.13703\ttraining's amex: 0.936403\tvalid_1's binary_logloss: 0.215188\tvalid_1's amex: 0.799585\n",
      "[10000]\ttraining's binary_logloss: 0.133383\ttraining's amex: 0.942133\tvalid_1's binary_logloss: 0.21508\tvalid_1's amex: 0.799085\n",
      "[10500]\ttraining's binary_logloss: 0.129809\ttraining's amex: 0.947749\tvalid_1's binary_logloss: 0.215051\tvalid_1's amex: 0.799587\n",
      "Our fold 3 CV score is 0.7993762721650746\n",
      "Number of estimator: 10500\n",
      "X_train: 367131\n",
      "X_val: 91782\n",
      "Y_train: 367131\n",
      "Y_validation: 91782\n",
      "Start Training fold 4\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.198409 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 529232\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 3830\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.291354\ttraining's amex: 0.777355\tvalid_1's binary_logloss: 0.295676\tvalid_1's amex: 0.765938\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [202]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m lgb_valid \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(x_val, y_val, categorical_feature\u001b[38;5;241m=\u001b[39mcat_features)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart Training fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlgb_valid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlgb_amex_metric\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Save best model\u001b[39;00m\n\u001b[1;32m     37\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(model, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./dart_models/model_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_seed\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/amex/lib/python3.10/site-packages/lightgbm/engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    285\u001b[0m     cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m    286\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    287\u001b[0m                             iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[1;32m    288\u001b[0m                             begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[1;32m    289\u001b[0m                             end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[1;32m    290\u001b[0m                             evaluation_result_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m--> 292\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m evaluation_result_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/amex/lib/python3.10/site-packages/lightgbm/basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 3021\u001b[0m _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3022\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold in range(5):\n",
    "    if fold < 4:\n",
    "        continue\n",
    "    n_estimator = n_est[fold]\n",
    "    print(f\"Number of estimator: {n_estimator}\")\n",
    "    xn = normal_train_agg.loc[normal_indices[fold][0]].drop(columns=not_in_x_columns)\n",
    "    xd = default_train_agg.loc[default_indices[fold][0]].drop(columns=not_in_x_columns)\n",
    "    x_train = pd.concat([xn, xd], ignore_index=True)\n",
    "    print(f\"X_train: {x_train.shape[0]}\")\n",
    "    xn = normal_train_agg.loc[normal_indices[fold][1]].drop(columns=not_in_x_columns)\n",
    "    xd = default_train_agg.loc[default_indices[fold][1]].drop(columns=not_in_x_columns)\n",
    "    x_val = pd.concat([xn, xd], ignore_index=True)\n",
    "    print(f\"X_val: {x_val.shape[0]}\")\n",
    "    \n",
    "    yn = normal_train_agg.loc[normal_indices[fold][0], \"target\"]\n",
    "    yd = default_train_agg.loc[default_indices[fold][0], \"target\"]\n",
    "    y_train = pd.concat([yn, yd], ignore_index=True)\n",
    "    print(f\"Y_train: {y_train.shape[0]}\")\n",
    "    \n",
    "    yn = normal_train_agg.loc[normal_indices[fold][1], \"target\"]\n",
    "    yd = default_train_agg.loc[default_indices[fold][1], \"target\"]\n",
    "    y_val = pd.concat([yn, yd], ignore_index=True)\n",
    "    print(f\"Y_validation: {y_val.shape[0]}\")\n",
    "    \n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "    print(f\"Start Training fold {fold}\")\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = n_estimator,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 600,\n",
    "        verbose_eval = 500,\n",
    "        feval = lgb_amex_metric\n",
    "    )\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'./dart_models/model_fold{fold}_seed{seed}.pkl')\n",
    "    # Predict validation\n",
    "    y_val_pred = model.predict(x_val, raw_score=True)\n",
    "    val_score, val_g, val_t4 = amex_metric(y_val, y_val_pred)                                      \n",
    "    print(f'Our fold {fold} CV score is {val_score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb40a3c-dc98-497f-917d-b6c21b4067c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af576609-fe47-4fe5-8882-64abf1978281",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71589a4e-47e0-44d7-ba68-6b6e1ecf109b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "75dd910d-e070-490e-ac5c-73c04f8724af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimator: 8000\n",
      "X_train: 367130\n",
      "X_val: 91783\n",
      "Y_train: 367130\n",
      "Y_validation: 91783\n",
      "Start Training fold 0\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.703449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 566106\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 4487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.288424\ttraining's amex: 0.780779\tvalid_1's binary_logloss: 0.294466\tvalid_1's amex: 0.764587\n",
      "[1000]\ttraining's binary_logloss: 0.230785\ttraining's amex: 0.798886\tvalid_1's binary_logloss: 0.241922\tvalid_1's amex: 0.776767\n",
      "[1500]\ttraining's binary_logloss: 0.215633\ttraining's amex: 0.813358\tvalid_1's binary_logloss: 0.231175\tvalid_1's amex: 0.782011\n",
      "[2000]\ttraining's binary_logloss: 0.205129\ttraining's amex: 0.825006\tvalid_1's binary_logloss: 0.225635\tvalid_1's amex: 0.786633\n",
      "[2500]\ttraining's binary_logloss: 0.196034\ttraining's amex: 0.836092\tvalid_1's binary_logloss: 0.222343\tvalid_1's amex: 0.789767\n",
      "[3000]\ttraining's binary_logloss: 0.189434\ttraining's amex: 0.846889\tvalid_1's binary_logloss: 0.220763\tvalid_1's amex: 0.791844\n",
      "[3500]\ttraining's binary_logloss: 0.183562\ttraining's amex: 0.85644\tvalid_1's binary_logloss: 0.219678\tvalid_1's amex: 0.792377\n",
      "[4000]\ttraining's binary_logloss: 0.17693\ttraining's amex: 0.867098\tvalid_1's binary_logloss: 0.218661\tvalid_1's amex: 0.793084\n",
      "[4500]\ttraining's binary_logloss: 0.170523\ttraining's amex: 0.87816\tvalid_1's binary_logloss: 0.218018\tvalid_1's amex: 0.793203\n",
      "[5000]\ttraining's binary_logloss: 0.164532\ttraining's amex: 0.888038\tvalid_1's binary_logloss: 0.21752\tvalid_1's amex: 0.793619\n",
      "[5500]\ttraining's binary_logloss: 0.159177\ttraining's amex: 0.89797\tvalid_1's binary_logloss: 0.217219\tvalid_1's amex: 0.793986\n",
      "[6000]\ttraining's binary_logloss: 0.154154\ttraining's amex: 0.906545\tvalid_1's binary_logloss: 0.216974\tvalid_1's amex: 0.793853\n",
      "[6500]\ttraining's binary_logloss: 0.149578\ttraining's amex: 0.914598\tvalid_1's binary_logloss: 0.216826\tvalid_1's amex: 0.793837\n",
      "[7000]\ttraining's binary_logloss: 0.144991\ttraining's amex: 0.922007\tvalid_1's binary_logloss: 0.216681\tvalid_1's amex: 0.794142\n",
      "[7500]\ttraining's binary_logloss: 0.140451\ttraining's amex: 0.929356\tvalid_1's binary_logloss: 0.216555\tvalid_1's amex: 0.794326\n",
      "[8000]\ttraining's binary_logloss: 0.136295\ttraining's amex: 0.936329\tvalid_1's binary_logloss: 0.216519\tvalid_1's amex: 0.793915\n",
      "Our fold 0 CV score is 0.7936758241645094\n",
      "Number of estimator: 8000\n",
      "X_train: 367130\n",
      "X_val: 91783\n",
      "Y_train: 367130\n",
      "Y_validation: 91783\n",
      "Start Training fold 1\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.485619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 565984\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 4487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.288824\ttraining's amex: 0.779613\tvalid_1's binary_logloss: 0.29344\tvalid_1's amex: 0.767531\n",
      "[1000]\ttraining's binary_logloss: 0.231481\ttraining's amex: 0.797812\tvalid_1's binary_logloss: 0.240571\tvalid_1's amex: 0.778705\n",
      "[1500]\ttraining's binary_logloss: 0.216172\ttraining's amex: 0.812111\tvalid_1's binary_logloss: 0.229635\tvalid_1's amex: 0.785421\n",
      "[2000]\ttraining's binary_logloss: 0.205678\ttraining's amex: 0.823271\tvalid_1's binary_logloss: 0.224113\tvalid_1's amex: 0.790013\n",
      "[2500]\ttraining's binary_logloss: 0.196458\ttraining's amex: 0.834874\tvalid_1's binary_logloss: 0.220626\tvalid_1's amex: 0.792453\n",
      "[3000]\ttraining's binary_logloss: 0.189884\ttraining's amex: 0.845843\tvalid_1's binary_logloss: 0.21917\tvalid_1's amex: 0.794975\n",
      "[3500]\ttraining's binary_logloss: 0.184006\ttraining's amex: 0.855708\tvalid_1's binary_logloss: 0.218147\tvalid_1's amex: 0.796505\n",
      "[4000]\ttraining's binary_logloss: 0.177344\ttraining's amex: 0.86595\tvalid_1's binary_logloss: 0.217212\tvalid_1's amex: 0.797027\n",
      "[4500]\ttraining's binary_logloss: 0.171003\ttraining's amex: 0.876685\tvalid_1's binary_logloss: 0.216594\tvalid_1's amex: 0.797929\n",
      "[5000]\ttraining's binary_logloss: 0.164984\ttraining's amex: 0.887028\tvalid_1's binary_logloss: 0.216052\tvalid_1's amex: 0.798158\n",
      "[5500]\ttraining's binary_logloss: 0.159661\ttraining's amex: 0.896739\tvalid_1's binary_logloss: 0.215721\tvalid_1's amex: 0.798064\n",
      "[6000]\ttraining's binary_logloss: 0.154643\ttraining's amex: 0.905866\tvalid_1's binary_logloss: 0.215484\tvalid_1's amex: 0.798241\n",
      "[6500]\ttraining's binary_logloss: 0.150088\ttraining's amex: 0.913327\tvalid_1's binary_logloss: 0.215362\tvalid_1's amex: 0.798761\n",
      "[7000]\ttraining's binary_logloss: 0.145536\ttraining's amex: 0.920973\tvalid_1's binary_logloss: 0.21516\tvalid_1's amex: 0.799394\n",
      "[7500]\ttraining's binary_logloss: 0.140991\ttraining's amex: 0.92884\tvalid_1's binary_logloss: 0.214947\tvalid_1's amex: 0.799061\n",
      "[8000]\ttraining's binary_logloss: 0.136779\ttraining's amex: 0.935589\tvalid_1's binary_logloss: 0.214848\tvalid_1's amex: 0.799032\n",
      "Our fold 1 CV score is 0.798792526003361\n",
      "Number of estimator: 8000\n",
      "X_train: 367130\n",
      "X_val: 91783\n",
      "Y_train: 367130\n",
      "Y_validation: 91783\n",
      "Start Training fold 2\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.428364 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 565919\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 4487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.288769\ttraining's amex: 0.779353\tvalid_1's binary_logloss: 0.293421\tvalid_1's amex: 0.770597\n",
      "[1000]\ttraining's binary_logloss: 0.231303\ttraining's amex: 0.798596\tvalid_1's binary_logloss: 0.240635\tvalid_1's amex: 0.779752\n",
      "[1500]\ttraining's binary_logloss: 0.216024\ttraining's amex: 0.812684\tvalid_1's binary_logloss: 0.229758\tvalid_1's amex: 0.786159\n",
      "[2000]\ttraining's binary_logloss: 0.20557\ttraining's amex: 0.823841\tvalid_1's binary_logloss: 0.224131\tvalid_1's amex: 0.790192\n",
      "[2500]\ttraining's binary_logloss: 0.196318\ttraining's amex: 0.835157\tvalid_1's binary_logloss: 0.220603\tvalid_1's amex: 0.792965\n",
      "[3000]\ttraining's binary_logloss: 0.189781\ttraining's amex: 0.846274\tvalid_1's binary_logloss: 0.219259\tvalid_1's amex: 0.795361\n",
      "[3500]\ttraining's binary_logloss: 0.183888\ttraining's amex: 0.85631\tvalid_1's binary_logloss: 0.218257\tvalid_1's amex: 0.796537\n",
      "[4000]\ttraining's binary_logloss: 0.177254\ttraining's amex: 0.866475\tvalid_1's binary_logloss: 0.217238\tvalid_1's amex: 0.797197\n",
      "[4500]\ttraining's binary_logloss: 0.170862\ttraining's amex: 0.877031\tvalid_1's binary_logloss: 0.216614\tvalid_1's amex: 0.797272\n",
      "[5000]\ttraining's binary_logloss: 0.164863\ttraining's amex: 0.887716\tvalid_1's binary_logloss: 0.216083\tvalid_1's amex: 0.797239\n",
      "[5500]\ttraining's binary_logloss: 0.159517\ttraining's amex: 0.897257\tvalid_1's binary_logloss: 0.215756\tvalid_1's amex: 0.797465\n",
      "[6000]\ttraining's binary_logloss: 0.154473\ttraining's amex: 0.906135\tvalid_1's binary_logloss: 0.21557\tvalid_1's amex: 0.798195\n",
      "[6500]\ttraining's binary_logloss: 0.149914\ttraining's amex: 0.914346\tvalid_1's binary_logloss: 0.215369\tvalid_1's amex: 0.798789\n",
      "[7000]\ttraining's binary_logloss: 0.145341\ttraining's amex: 0.921413\tvalid_1's binary_logloss: 0.215249\tvalid_1's amex: 0.798572\n",
      "[7500]\ttraining's binary_logloss: 0.140808\ttraining's amex: 0.92858\tvalid_1's binary_logloss: 0.215016\tvalid_1's amex: 0.798842\n",
      "[8000]\ttraining's binary_logloss: 0.136612\ttraining's amex: 0.935406\tvalid_1's binary_logloss: 0.214889\tvalid_1's amex: 0.798599\n",
      "Our fold 2 CV score is 0.7983607073892527\n",
      "Number of estimator: 8000\n",
      "X_train: 367131\n",
      "X_val: 91782\n",
      "Y_train: 367131\n",
      "Y_validation: 91782\n",
      "Start Training fold 3\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.381134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 565929\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 4487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.288854\ttraining's amex: 0.779436\tvalid_1's binary_logloss: 0.292844\tvalid_1's amex: 0.769673\n",
      "[1000]\ttraining's binary_logloss: 0.231314\ttraining's amex: 0.798558\tvalid_1's binary_logloss: 0.239894\tvalid_1's amex: 0.783305\n",
      "[1500]\ttraining's binary_logloss: 0.216168\ttraining's amex: 0.812184\tvalid_1's binary_logloss: 0.229099\tvalid_1's amex: 0.787676\n",
      "[2000]\ttraining's binary_logloss: 0.205676\ttraining's amex: 0.823267\tvalid_1's binary_logloss: 0.223414\tvalid_1's amex: 0.791182\n",
      "[2500]\ttraining's binary_logloss: 0.19651\ttraining's amex: 0.83546\tvalid_1's binary_logloss: 0.219886\tvalid_1's amex: 0.793223\n",
      "[3000]\ttraining's binary_logloss: 0.189928\ttraining's amex: 0.845797\tvalid_1's binary_logloss: 0.218313\tvalid_1's amex: 0.795373\n",
      "[3500]\ttraining's binary_logloss: 0.184113\ttraining's amex: 0.855742\tvalid_1's binary_logloss: 0.217322\tvalid_1's amex: 0.796092\n",
      "[4000]\ttraining's binary_logloss: 0.177418\ttraining's amex: 0.865991\tvalid_1's binary_logloss: 0.216289\tvalid_1's amex: 0.798172\n",
      "[4500]\ttraining's binary_logloss: 0.170998\ttraining's amex: 0.877099\tvalid_1's binary_logloss: 0.215678\tvalid_1's amex: 0.797775\n",
      "[5000]\ttraining's binary_logloss: 0.165008\ttraining's amex: 0.887348\tvalid_1's binary_logloss: 0.215258\tvalid_1's amex: 0.798713\n",
      "[5500]\ttraining's binary_logloss: 0.159658\ttraining's amex: 0.897049\tvalid_1's binary_logloss: 0.214986\tvalid_1's amex: 0.79865\n",
      "[6000]\ttraining's binary_logloss: 0.154632\ttraining's amex: 0.906379\tvalid_1's binary_logloss: 0.214834\tvalid_1's amex: 0.798559\n",
      "[6500]\ttraining's binary_logloss: 0.15007\ttraining's amex: 0.913915\tvalid_1's binary_logloss: 0.214709\tvalid_1's amex: 0.798744\n",
      "[7000]\ttraining's binary_logloss: 0.145483\ttraining's amex: 0.921401\tvalid_1's binary_logloss: 0.214535\tvalid_1's amex: 0.798899\n",
      "[7500]\ttraining's binary_logloss: 0.140948\ttraining's amex: 0.928409\tvalid_1's binary_logloss: 0.214446\tvalid_1's amex: 0.798619\n",
      "[8000]\ttraining's binary_logloss: 0.136793\ttraining's amex: 0.935088\tvalid_1's binary_logloss: 0.214442\tvalid_1's amex: 0.798402\n",
      "Our fold 3 CV score is 0.7981916621227327\n",
      "Number of estimator: 8000\n",
      "X_train: 367131\n",
      "X_val: 91782\n",
      "Y_train: 367131\n",
      "Y_validation: 91782\n",
      "Start Training fold 4\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.344941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 565933\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 4487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.288663\ttraining's amex: 0.77984\tvalid_1's binary_logloss: 0.293315\tvalid_1's amex: 0.767083\n",
      "[1000]\ttraining's binary_logloss: 0.231094\ttraining's amex: 0.798448\tvalid_1's binary_logloss: 0.240738\tvalid_1's amex: 0.779212\n",
      "[1500]\ttraining's binary_logloss: 0.215836\ttraining's amex: 0.812094\tvalid_1's binary_logloss: 0.229875\tvalid_1's amex: 0.785753\n",
      "[2000]\ttraining's binary_logloss: 0.205342\ttraining's amex: 0.82348\tvalid_1's binary_logloss: 0.224272\tvalid_1's amex: 0.78986\n",
      "[2500]\ttraining's binary_logloss: 0.1962\ttraining's amex: 0.83535\tvalid_1's binary_logloss: 0.220925\tvalid_1's amex: 0.792608\n",
      "[3000]\ttraining's binary_logloss: 0.189629\ttraining's amex: 0.845638\tvalid_1's binary_logloss: 0.219416\tvalid_1's amex: 0.79418\n",
      "[3500]\ttraining's binary_logloss: 0.183779\ttraining's amex: 0.855616\tvalid_1's binary_logloss: 0.21837\tvalid_1's amex: 0.795608\n",
      "[4000]\ttraining's binary_logloss: 0.177151\ttraining's amex: 0.865959\tvalid_1's binary_logloss: 0.217472\tvalid_1's amex: 0.796249\n",
      "[4500]\ttraining's binary_logloss: 0.170734\ttraining's amex: 0.877301\tvalid_1's binary_logloss: 0.216927\tvalid_1's amex: 0.796006\n",
      "[5000]\ttraining's binary_logloss: 0.164777\ttraining's amex: 0.887215\tvalid_1's binary_logloss: 0.216483\tvalid_1's amex: 0.795919\n",
      "[5500]\ttraining's binary_logloss: 0.159377\ttraining's amex: 0.897635\tvalid_1's binary_logloss: 0.21623\tvalid_1's amex: 0.796048\n",
      "[6000]\ttraining's binary_logloss: 0.154325\ttraining's amex: 0.906239\tvalid_1's binary_logloss: 0.216003\tvalid_1's amex: 0.795693\n",
      "[6500]\ttraining's binary_logloss: 0.149759\ttraining's amex: 0.914282\tvalid_1's binary_logloss: 0.215863\tvalid_1's amex: 0.795253\n",
      "[7000]\ttraining's binary_logloss: 0.145208\ttraining's amex: 0.921555\tvalid_1's binary_logloss: 0.215754\tvalid_1's amex: 0.795391\n",
      "[7500]\ttraining's binary_logloss: 0.140668\ttraining's amex: 0.929315\tvalid_1's binary_logloss: 0.215591\tvalid_1's amex: 0.795466\n",
      "[8000]\ttraining's binary_logloss: 0.136456\ttraining's amex: 0.935854\tvalid_1's binary_logloss: 0.215484\tvalid_1's amex: 0.795113\n",
      "Our fold 4 CV score is 0.7949018082701897\n"
     ]
    }
   ],
   "source": [
    "for fold in range(5):\n",
    "    \n",
    "    n_estimator = n_est[fold]\n",
    "    print(f\"Number of estimator: {n_estimator}\")\n",
    "    xn = normal_train_agg.loc[normal_indices[fold][0]].drop(columns=not_in_x_columns)\n",
    "    xd = default_train_agg.loc[default_indices[fold][0]].drop(columns=not_in_x_columns)\n",
    "    x_train = pd.concat([xn, xd], ignore_index=True)\n",
    "    print(f\"X_train: {x_train.shape[0]}\")\n",
    "    xn = normal_train_agg.loc[normal_indices[fold][1]].drop(columns=not_in_x_columns)\n",
    "    xd = default_train_agg.loc[default_indices[fold][1]].drop(columns=not_in_x_columns)\n",
    "    x_val = pd.concat([xn, xd], ignore_index=True)\n",
    "    print(f\"X_val: {x_val.shape[0]}\")\n",
    "    \n",
    "    yn = normal_train_agg.loc[normal_indices[fold][0], \"target\"]\n",
    "    yd = default_train_agg.loc[default_indices[fold][0], \"target\"]\n",
    "    y_train = pd.concat([yn, yd], ignore_index=True)\n",
    "    print(f\"Y_train: {y_train.shape[0]}\")\n",
    "    \n",
    "    yn = normal_train_agg.loc[normal_indices[fold][1], \"target\"]\n",
    "    yd = default_train_agg.loc[default_indices[fold][1], \"target\"]\n",
    "    y_val = pd.concat([yn, yd], ignore_index=True)\n",
    "    print(f\"Y_validation: {y_val.shape[0]}\")\n",
    "    \n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "    print(f\"Start Training fold {fold}\")\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = n_estimator,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 600,\n",
    "        verbose_eval = 500,\n",
    "        feval = lgb_amex_metric\n",
    "    )\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'./dart_models/model_fold{fold}_seed{seed}.pkl')\n",
    "    # Predict validation\n",
    "    y_val_pred = model.predict(x_val, raw_score=True)\n",
    "    val_score, val_g, val_t4 = amex_metric(y_val, y_val_pred)                                      \n",
    "    print(f'Our fold {fold} CV score is {val_score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735a0301-0cc4-4392-9a65-76856ed30744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 3983 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 3\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.258323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 553242\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 3983\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.265894\ttraining's amex: 0.784579\tvalid_1's binary_logloss: 0.270852\tvalid_1's amex: 0.776311\n",
      "[1000]\ttraining's binary_logloss: 0.220583\ttraining's amex: 0.804852\tvalid_1's binary_logloss: 0.231342\tvalid_1's amex: 0.786518\n",
      "[1500]\ttraining's binary_logloss: 0.205965\ttraining's amex: 0.821195\tvalid_1's binary_logloss: 0.22299\tvalid_1's amex: 0.793104\n",
      "[2000]\ttraining's binary_logloss: 0.197101\ttraining's amex: 0.835248\tvalid_1's binary_logloss: 0.220033\tvalid_1's amex: 0.795629\n",
      "[2500]\ttraining's binary_logloss: 0.187383\ttraining's amex: 0.848788\tvalid_1's binary_logloss: 0.217661\tvalid_1's amex: 0.796793\n"
     ]
    }
   ],
   "source": [
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_agg, target)):\n",
    "    if fold < 3:\n",
    "        continue\n",
    "    n_estimator = n_est[fold]\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {train_agg.shape[1]} features...')\n",
    "    print('-'*50)\n",
    "    x_train, x_val = train_agg.iloc[trn_ind], train_agg.iloc[val_ind]\n",
    "    y_train, y_val = target[trn_ind], target[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "    print(f\"Start Training fold {fold}\")\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = n_estimator,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 600,\n",
    "        verbose_eval = 500,\n",
    "        feval = lgb_amex_metric\n",
    "    )\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'./dart_models/model_fold{fold}_seed{seed}.pkl')\n",
    "    # Predict validation\n",
    "    y_val_pred = model.predict(x_val, raw_score=True)\n",
    "    val_score, val_g, val_t4 = amex_metric(y_val, y_val_pred)                                      \n",
    "    print(f'Our fold {fold} CV score is {val_score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04680e2b-df75-44f4-a2f1-6c2c0f90f004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

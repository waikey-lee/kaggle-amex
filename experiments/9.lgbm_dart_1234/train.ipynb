{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd2ca34-3410-4078-aa48-7adbba211ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import joblib\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import os\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from itertools import repeat\n",
    "from lightgbm import LGBMClassifier, log_evaluation\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, fbeta_score, make_scorer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb17d64c-4cdb-4906-be84-1348484a4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import ListedColormap\n",
    "from cycler import cycler\n",
    "from IPython.display import display\n",
    "from colorama import Fore, Back, Style\n",
    "plt.rcParams['axes.facecolor'] = '#0057b8' # blue\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=['#ffd700'] +\n",
    "                                         plt.rcParams['axes.prop_cycle'].by_key()['color'][1:])\n",
    "plt.rcParams['text.color'] = 'w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a084bef6-4434-40fb-bafc-d7d3e4c57a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.common import (\n",
    "    sigmoid, pad_column_name\n",
    ")\n",
    "from utils.constants import *\n",
    "from utils.eval_helpers import (\n",
    "    plot_roc_curves, plot_feature_importance, \n",
    "    amex_metric, get_final_metric_df, amex_metric_np, lgb_amex_metric\n",
    ")\n",
    "from utils.eda_helpers import (\n",
    "    plot_missing_proportion_barchart, \n",
    "    get_cols\n",
    ")\n",
    "from utils.extraction_helpers import read_file\n",
    "from utils.feature_group import (\n",
    "    CATEGORY_COLUMNS, CONTINUOUS_COLUMNS, NON_FEATURE_COLUMNS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67194602-57fc-4e92-9801-8d600e52a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885beac-e245-43b6-a2cc-73e9f871d132",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6d46155-ea8e-4212-8c63-5b7ac8b08d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (458913, 4241)\n",
      "CPU times: user 14.1 s, sys: 19 s, total: 33.1 s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_agg = read_file(f\"../{PROCESSED_DATA_PATH}/v5/final_train_agg.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9c7632d-3cf9-4ff9-9467-9ebe9d2daecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (458913, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = read_file(f\"../{RAW_DATA_PATH}/train_labels.csv\")\n",
    "target = labels[\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46942b4a-fd5a-4bd0-9dbd-7ceb4b99f5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 467 ms, sys: 2.85 s, total: 3.32 s\n",
      "Wall time: 5.49 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_agg = train_agg.drop(columns=NON_FEATURE_COLUMNS + [\"target\"], errors=\"ignore\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffe8f01a-341d-4cbe-8c46-8ae026830c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D_92_last', 'D_114_first', 'D_68_third_last', 'D_63_third_last', 'D_92_first', 'D_126_first', 'D_116_third_last', 'D_63_last', 'D_92_second_last', 'D_126_third_last', 'B_30_first', 'D_117_third_last', 'B_38_first', 'D_117_last', 'D_68_first', 'D_114_last', 'D_117_second_last', 'D_64_third_last', 'D_120_second_last', 'D_116_first', 'D_63_first', 'B_38_last', 'B_38_third_last', 'D_114_second_last', 'D_126_last', 'B_30_second_last', 'D_64_last', 'D_120_first', 'D_116_last', 'D_64_second_last', 'B_30_third_last', 'D_120_last', 'D_64_first', 'B_38_second_last', 'D_126_second_last', 'D_116_second_last', 'B_30_last', 'D_117_first', 'D_68_second_last', 'D_114_third_last', 'D_120_third_last', 'D_63_second_last', 'D_92_third_last', 'D_68_last']\n"
     ]
    }
   ],
   "source": [
    "cat_columns = get_cols(train_agg, CATEGORY_COLUMNS)\n",
    "cat_features = list(set(cat_columns).intersection(train_agg.columns))\n",
    "print(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80925934-1a9e-4dda-b8dc-4557eb8c6d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((458913, 4336), (458913,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_agg.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63209e65-9d82-4b2e-bf34-f4a15560f08f",
   "metadata": {},
   "source": [
    "### Train LGBM using pre-set hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3d2e782-8f4d-4e59-a03e-e962e88db2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4242c36-4936-45c9-a8e7-33ff9a312586",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': \"binary_logloss\",\n",
    "    'boosting': 'dart',\n",
    "    'device': \"cpu\",\n",
    "    'seed': seed,\n",
    "    'num_leaves': 90,\n",
    "    'learning_rate': 0.0115,\n",
    "    'feature_fraction': 0.19,\n",
    "    'bagging_freq': 8,\n",
    "    'bagging_fraction': 0.55,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l2': 18,\n",
    "    'min_data_in_leaf': 75,\n",
    "    'scale_pos_weight': 1.4,\n",
    "    'max_bins': 255\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c57a3a3d-a51c-436e-9b0b-d19a66668b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29b170e8-eac9-4d09-970d-14244f215eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_est = [9000, 9000, 6500, 7500, 7000]\n",
    "n_est = [10000] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3491b4e8-78bc-4f47-bb8d-2fac68019bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 4240 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 0\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.120198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 554491\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 4224\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.316735\ttraining's amex: 0.771707\tvalid_1's binary_logloss: 0.320749\tvalid_1's amex: 0.762915\n",
      "[1000]\ttraining's binary_logloss: 0.248487\ttraining's amex: 0.789107\tvalid_1's binary_logloss: 0.256228\tvalid_1's amex: 0.77677\n",
      "[1500]\ttraining's binary_logloss: 0.223322\ttraining's amex: 0.803181\tvalid_1's binary_logloss: 0.235333\tvalid_1's amex: 0.784146\n",
      "[2000]\ttraining's binary_logloss: 0.21141\ttraining's amex: 0.816798\tvalid_1's binary_logloss: 0.228028\tvalid_1's amex: 0.788818\n",
      "[2500]\ttraining's binary_logloss: 0.203946\ttraining's amex: 0.82704\tvalid_1's binary_logloss: 0.224865\tvalid_1's amex: 0.79128\n",
      "[3000]\ttraining's binary_logloss: 0.197121\ttraining's amex: 0.837218\tvalid_1's binary_logloss: 0.222739\tvalid_1's amex: 0.792856\n",
      "[3500]\ttraining's binary_logloss: 0.190764\ttraining's amex: 0.846661\tvalid_1's binary_logloss: 0.221185\tvalid_1's amex: 0.794479\n",
      "[4000]\ttraining's binary_logloss: 0.184651\ttraining's amex: 0.855701\tvalid_1's binary_logloss: 0.220215\tvalid_1's amex: 0.795621\n",
      "[4500]\ttraining's binary_logloss: 0.179196\ttraining's amex: 0.864897\tvalid_1's binary_logloss: 0.219525\tvalid_1's amex: 0.795561\n",
      "[5000]\ttraining's binary_logloss: 0.17459\ttraining's amex: 0.873025\tvalid_1's binary_logloss: 0.219143\tvalid_1's amex: 0.795571\n",
      "[5500]\ttraining's binary_logloss: 0.169873\ttraining's amex: 0.881194\tvalid_1's binary_logloss: 0.218788\tvalid_1's amex: 0.796676\n",
      "[6000]\ttraining's binary_logloss: 0.165053\ttraining's amex: 0.889109\tvalid_1's binary_logloss: 0.218483\tvalid_1's amex: 0.797072\n",
      "[6500]\ttraining's binary_logloss: 0.161077\ttraining's amex: 0.896966\tvalid_1's binary_logloss: 0.218279\tvalid_1's amex: 0.797618\n",
      "[7000]\ttraining's binary_logloss: 0.156545\ttraining's amex: 0.904635\tvalid_1's binary_logloss: 0.218052\tvalid_1's amex: 0.797858\n",
      "[7500]\ttraining's binary_logloss: 0.151755\ttraining's amex: 0.912648\tvalid_1's binary_logloss: 0.217825\tvalid_1's amex: 0.797885\n",
      "[8000]\ttraining's binary_logloss: 0.147943\ttraining's amex: 0.919089\tvalid_1's binary_logloss: 0.217722\tvalid_1's amex: 0.797818\n",
      "[8500]\ttraining's binary_logloss: 0.144398\ttraining's amex: 0.925368\tvalid_1's binary_logloss: 0.217642\tvalid_1's amex: 0.798176\n",
      "[9000]\ttraining's binary_logloss: 0.140615\ttraining's amex: 0.931377\tvalid_1's binary_logloss: 0.217621\tvalid_1's amex: 0.797688\n",
      "[9500]\ttraining's binary_logloss: 0.137303\ttraining's amex: 0.936772\tvalid_1's binary_logloss: 0.217608\tvalid_1's amex: 0.797525\n",
      "[10000]\ttraining's binary_logloss: 0.133992\ttraining's amex: 0.942079\tvalid_1's binary_logloss: 0.217597\tvalid_1's amex: 0.797199\n",
      "Our fold 0 CV score is 0.7969598088907677\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 4240 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 1\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.194634 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 554721\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 4225\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.316803\ttraining's amex: 0.77177\tvalid_1's binary_logloss: 0.32044\tvalid_1's amex: 0.764984\n",
      "[1000]\ttraining's binary_logloss: 0.248939\ttraining's amex: 0.789019\tvalid_1's binary_logloss: 0.256163\tvalid_1's amex: 0.775449\n",
      "[1500]\ttraining's binary_logloss: 0.223697\ttraining's amex: 0.803557\tvalid_1's binary_logloss: 0.235073\tvalid_1's amex: 0.784144\n",
      "[2000]\ttraining's binary_logloss: 0.211726\ttraining's amex: 0.815861\tvalid_1's binary_logloss: 0.227702\tvalid_1's amex: 0.788856\n",
      "[2500]\ttraining's binary_logloss: 0.204308\ttraining's amex: 0.825935\tvalid_1's binary_logloss: 0.224452\tvalid_1's amex: 0.791678\n",
      "[3000]\ttraining's binary_logloss: 0.197467\ttraining's amex: 0.836076\tvalid_1's binary_logloss: 0.222221\tvalid_1's amex: 0.793481\n",
      "[3500]\ttraining's binary_logloss: 0.191161\ttraining's amex: 0.845528\tvalid_1's binary_logloss: 0.220624\tvalid_1's amex: 0.795621\n",
      "[4000]\ttraining's binary_logloss: 0.18503\ttraining's amex: 0.854852\tvalid_1's binary_logloss: 0.219489\tvalid_1's amex: 0.796604\n",
      "[4500]\ttraining's binary_logloss: 0.179567\ttraining's amex: 0.864018\tvalid_1's binary_logloss: 0.218771\tvalid_1's amex: 0.797609\n",
      "[5000]\ttraining's binary_logloss: 0.174917\ttraining's amex: 0.873133\tvalid_1's binary_logloss: 0.218295\tvalid_1's amex: 0.797308\n",
      "[5500]\ttraining's binary_logloss: 0.170276\ttraining's amex: 0.880683\tvalid_1's binary_logloss: 0.217902\tvalid_1's amex: 0.798664\n",
      "[6000]\ttraining's binary_logloss: 0.165487\ttraining's amex: 0.888698\tvalid_1's binary_logloss: 0.217517\tvalid_1's amex: 0.799031\n",
      "[6500]\ttraining's binary_logloss: 0.161485\ttraining's amex: 0.896306\tvalid_1's binary_logloss: 0.217261\tvalid_1's amex: 0.799347\n",
      "[7000]\ttraining's binary_logloss: 0.156948\ttraining's amex: 0.904118\tvalid_1's binary_logloss: 0.217023\tvalid_1's amex: 0.800553\n",
      "[7500]\ttraining's binary_logloss: 0.152201\ttraining's amex: 0.911918\tvalid_1's binary_logloss: 0.216684\tvalid_1's amex: 0.800642\n",
      "[8000]\ttraining's binary_logloss: 0.148379\ttraining's amex: 0.918524\tvalid_1's binary_logloss: 0.216572\tvalid_1's amex: 0.799387\n",
      "[8500]\ttraining's binary_logloss: 0.144881\ttraining's amex: 0.925133\tvalid_1's binary_logloss: 0.216437\tvalid_1's amex: 0.799704\n",
      "[9000]\ttraining's binary_logloss: 0.141096\ttraining's amex: 0.931087\tvalid_1's binary_logloss: 0.216323\tvalid_1's amex: 0.799455\n",
      "[9500]\ttraining's binary_logloss: 0.137756\ttraining's amex: 0.936372\tvalid_1's binary_logloss: 0.216262\tvalid_1's amex: 0.80039\n",
      "[10000]\ttraining's binary_logloss: 0.134432\ttraining's amex: 0.941585\tvalid_1's binary_logloss: 0.216224\tvalid_1's amex: 0.800505\n",
      "Our fold 1 CV score is 0.8002663458916823\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 4240 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 2\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.967140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 554198\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 4224\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.317387\ttraining's amex: 0.770798\tvalid_1's binary_logloss: 0.319095\tvalid_1's amex: 0.767529\n",
      "[1000]\ttraining's binary_logloss: 0.249353\ttraining's amex: 0.78901\tvalid_1's binary_logloss: 0.253969\tvalid_1's amex: 0.779971\n",
      "[1500]\ttraining's binary_logloss: 0.224213\ttraining's amex: 0.802771\tvalid_1's binary_logloss: 0.232659\tvalid_1's amex: 0.786525\n",
      "[2000]\ttraining's binary_logloss: 0.212282\ttraining's amex: 0.815491\tvalid_1's binary_logloss: 0.22524\tvalid_1's amex: 0.790736\n",
      "[2500]\ttraining's binary_logloss: 0.204822\ttraining's amex: 0.825594\tvalid_1's binary_logloss: 0.221928\tvalid_1's amex: 0.793274\n",
      "[3000]\ttraining's binary_logloss: 0.198034\ttraining's amex: 0.835594\tvalid_1's binary_logloss: 0.219703\tvalid_1's amex: 0.795653\n",
      "[3500]\ttraining's binary_logloss: 0.191722\ttraining's amex: 0.845558\tvalid_1's binary_logloss: 0.218087\tvalid_1's amex: 0.796919\n",
      "[4000]\ttraining's binary_logloss: 0.185507\ttraining's amex: 0.855048\tvalid_1's binary_logloss: 0.216816\tvalid_1's amex: 0.798283\n",
      "[4500]\ttraining's binary_logloss: 0.180073\ttraining's amex: 0.863979\tvalid_1's binary_logloss: 0.216048\tvalid_1's amex: 0.798569\n",
      "[5000]\ttraining's binary_logloss: 0.175418\ttraining's amex: 0.872397\tvalid_1's binary_logloss: 0.215605\tvalid_1's amex: 0.799204\n",
      "[5500]\ttraining's binary_logloss: 0.170664\ttraining's amex: 0.880471\tvalid_1's binary_logloss: 0.215079\tvalid_1's amex: 0.799089\n",
      "[6000]\ttraining's binary_logloss: 0.165923\ttraining's amex: 0.888339\tvalid_1's binary_logloss: 0.214813\tvalid_1's amex: 0.79927\n",
      "[6500]\ttraining's binary_logloss: 0.161934\ttraining's amex: 0.896024\tvalid_1's binary_logloss: 0.214579\tvalid_1's amex: 0.799266\n",
      "[7000]\ttraining's binary_logloss: 0.157358\ttraining's amex: 0.903451\tvalid_1's binary_logloss: 0.214185\tvalid_1's amex: 0.799502\n",
      "[7500]\ttraining's binary_logloss: 0.152671\ttraining's amex: 0.911309\tvalid_1's binary_logloss: 0.213969\tvalid_1's amex: 0.798795\n",
      "[8000]\ttraining's binary_logloss: 0.14883\ttraining's amex: 0.918353\tvalid_1's binary_logloss: 0.213737\tvalid_1's amex: 0.798975\n",
      "[8500]\ttraining's binary_logloss: 0.145318\ttraining's amex: 0.924577\tvalid_1's binary_logloss: 0.213624\tvalid_1's amex: 0.798985\n",
      "[9000]\ttraining's binary_logloss: 0.141483\ttraining's amex: 0.93054\tvalid_1's binary_logloss: 0.213468\tvalid_1's amex: 0.798944\n",
      "[9500]\ttraining's binary_logloss: 0.138182\ttraining's amex: 0.936173\tvalid_1's binary_logloss: 0.213361\tvalid_1's amex: 0.799603\n",
      "[10000]\ttraining's binary_logloss: 0.134877\ttraining's amex: 0.941055\tvalid_1's binary_logloss: 0.213251\tvalid_1's amex: 0.799697\n",
      "Our fold 2 CV score is 0.7994580791412472\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 4240 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 3\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.032828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 554499\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 4225\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.316911\ttraining's amex: 0.773012\tvalid_1's binary_logloss: 0.319663\tvalid_1's amex: 0.762558\n",
      "[1000]\ttraining's binary_logloss: 0.249235\ttraining's amex: 0.790138\tvalid_1's binary_logloss: 0.254752\tvalid_1's amex: 0.773277\n",
      "[1500]\ttraining's binary_logloss: 0.224165\ttraining's amex: 0.803582\tvalid_1's binary_logloss: 0.233174\tvalid_1's amex: 0.781837\n",
      "[2000]\ttraining's binary_logloss: 0.212188\ttraining's amex: 0.816092\tvalid_1's binary_logloss: 0.225318\tvalid_1's amex: 0.788124\n",
      "[2500]\ttraining's binary_logloss: 0.204749\ttraining's amex: 0.825932\tvalid_1's binary_logloss: 0.222017\tvalid_1's amex: 0.790973\n",
      "[3000]\ttraining's binary_logloss: 0.197931\ttraining's amex: 0.836146\tvalid_1's binary_logloss: 0.219893\tvalid_1's amex: 0.794223\n",
      "[3500]\ttraining's binary_logloss: 0.19163\ttraining's amex: 0.845453\tvalid_1's binary_logloss: 0.218396\tvalid_1's amex: 0.795002\n",
      "[4000]\ttraining's binary_logloss: 0.185478\ttraining's amex: 0.854644\tvalid_1's binary_logloss: 0.217257\tvalid_1's amex: 0.796599\n",
      "[4500]\ttraining's binary_logloss: 0.180012\ttraining's amex: 0.86382\tvalid_1's binary_logloss: 0.216507\tvalid_1's amex: 0.79687\n",
      "[5000]\ttraining's binary_logloss: 0.175396\ttraining's amex: 0.872162\tvalid_1's binary_logloss: 0.216017\tvalid_1's amex: 0.798406\n",
      "[5500]\ttraining's binary_logloss: 0.170722\ttraining's amex: 0.880419\tvalid_1's binary_logloss: 0.215612\tvalid_1's amex: 0.798202\n",
      "[6000]\ttraining's binary_logloss: 0.165889\ttraining's amex: 0.888747\tvalid_1's binary_logloss: 0.215162\tvalid_1's amex: 0.79877\n",
      "[6500]\ttraining's binary_logloss: 0.161915\ttraining's amex: 0.89621\tvalid_1's binary_logloss: 0.215002\tvalid_1's amex: 0.799124\n",
      "[7000]\ttraining's binary_logloss: 0.157369\ttraining's amex: 0.903661\tvalid_1's binary_logloss: 0.214748\tvalid_1's amex: 0.798606\n",
      "[7500]\ttraining's binary_logloss: 0.152618\ttraining's amex: 0.910894\tvalid_1's binary_logloss: 0.214447\tvalid_1's amex: 0.798933\n",
      "[8000]\ttraining's binary_logloss: 0.148829\ttraining's amex: 0.917785\tvalid_1's binary_logloss: 0.214311\tvalid_1's amex: 0.799269\n",
      "[8500]\ttraining's binary_logloss: 0.145303\ttraining's amex: 0.924049\tvalid_1's binary_logloss: 0.214159\tvalid_1's amex: 0.799289\n",
      "[9000]\ttraining's binary_logloss: 0.141515\ttraining's amex: 0.929826\tvalid_1's binary_logloss: 0.214026\tvalid_1's amex: 0.799089\n",
      "[9500]\ttraining's binary_logloss: 0.138175\ttraining's amex: 0.935585\tvalid_1's binary_logloss: 0.213937\tvalid_1's amex: 0.798902\n",
      "[10000]\ttraining's binary_logloss: 0.134846\ttraining's amex: 0.94067\tvalid_1's binary_logloss: 0.213844\tvalid_1's amex: 0.799396\n",
      "Our fold 3 CV score is 0.7991855726180278\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 4240 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 4\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.201899 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 554465\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 4224\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.315927\ttraining's amex: 0.773959\tvalid_1's binary_logloss: 0.322224\tvalid_1's amex: 0.756702\n",
      "[1000]\ttraining's binary_logloss: 0.247726\ttraining's amex: 0.791295\tvalid_1's binary_logloss: 0.258573\tvalid_1's amex: 0.768924\n",
      "[1500]\ttraining's binary_logloss: 0.222628\ttraining's amex: 0.804917\tvalid_1's binary_logloss: 0.238003\tvalid_1's amex: 0.776374\n",
      "[2000]\ttraining's binary_logloss: 0.210646\ttraining's amex: 0.817631\tvalid_1's binary_logloss: 0.230764\tvalid_1's amex: 0.781602\n",
      "[2500]\ttraining's binary_logloss: 0.20315\ttraining's amex: 0.826954\tvalid_1's binary_logloss: 0.227592\tvalid_1's amex: 0.784604\n",
      "[3000]\ttraining's binary_logloss: 0.196329\ttraining's amex: 0.837653\tvalid_1's binary_logloss: 0.225531\tvalid_1's amex: 0.785691\n",
      "[3500]\ttraining's binary_logloss: 0.190021\ttraining's amex: 0.847124\tvalid_1's binary_logloss: 0.224031\tvalid_1's amex: 0.788005\n",
      "[4000]\ttraining's binary_logloss: 0.183892\ttraining's amex: 0.856568\tvalid_1's binary_logloss: 0.223028\tvalid_1's amex: 0.788324\n",
      "[4500]\ttraining's binary_logloss: 0.178462\ttraining's amex: 0.865857\tvalid_1's binary_logloss: 0.222273\tvalid_1's amex: 0.789782\n",
      "[5000]\ttraining's binary_logloss: 0.173854\ttraining's amex: 0.874832\tvalid_1's binary_logloss: 0.221821\tvalid_1's amex: 0.790476\n",
      "[5500]\ttraining's binary_logloss: 0.169195\ttraining's amex: 0.882539\tvalid_1's binary_logloss: 0.221473\tvalid_1's amex: 0.790686\n",
      "[6000]\ttraining's binary_logloss: 0.164365\ttraining's amex: 0.890603\tvalid_1's binary_logloss: 0.221106\tvalid_1's amex: 0.790876\n",
      "[6500]\ttraining's binary_logloss: 0.16042\ttraining's amex: 0.898124\tvalid_1's binary_logloss: 0.220924\tvalid_1's amex: 0.791255\n",
      "[7000]\ttraining's binary_logloss: 0.155844\ttraining's amex: 0.905572\tvalid_1's binary_logloss: 0.220701\tvalid_1's amex: 0.790572\n",
      "[7500]\ttraining's binary_logloss: 0.151126\ttraining's amex: 0.912808\tvalid_1's binary_logloss: 0.220448\tvalid_1's amex: 0.790654\n",
      "[8000]\ttraining's binary_logloss: 0.147296\ttraining's amex: 0.919663\tvalid_1's binary_logloss: 0.220273\tvalid_1's amex: 0.791439\n",
      "[8500]\ttraining's binary_logloss: 0.143787\ttraining's amex: 0.926228\tvalid_1's binary_logloss: 0.220173\tvalid_1's amex: 0.791327\n",
      "[9000]\ttraining's binary_logloss: 0.140016\ttraining's amex: 0.932354\tvalid_1's binary_logloss: 0.22013\tvalid_1's amex: 0.790951\n",
      "[9500]\ttraining's binary_logloss: 0.136721\ttraining's amex: 0.937767\tvalid_1's binary_logloss: 0.220082\tvalid_1's amex: 0.790824\n",
      "[10000]\ttraining's binary_logloss: 0.13339\ttraining's amex: 0.942509\tvalid_1's binary_logloss: 0.220002\tvalid_1's amex: 0.791298\n",
      "Our fold 4 CV score is 0.7910863703517701\n"
     ]
    }
   ],
   "source": [
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_agg, target)):\n",
    "    n_estimator = n_est[fold]\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {train_agg.shape[1]} features...')\n",
    "    print('-'*50)\n",
    "    x_train, x_val = train_agg.iloc[trn_ind], train_agg.iloc[val_ind]\n",
    "    y_train, y_val = target[trn_ind], target[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "    print(f\"Start Training fold {fold}\")\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = n_estimator,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 300,\n",
    "        verbose_eval = 500,\n",
    "        feval = lgb_amex_metric\n",
    "    )\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'./models/model_fold{fold}_seed{seed}.pkl')\n",
    "    # Predict validation\n",
    "    y_val_pred = model.predict(x_val, raw_score=True)\n",
    "    val_score, val_g, val_t4 = amex_metric(y_val, y_val_pred)                                      \n",
    "    print(f'Our fold {fold} CV score is {val_score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce98a1ae-b845-44a9-b037-e7a0fdcada63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 2905 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 1\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.664060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 905033\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 2904\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[1000]\ttraining's binary_logloss: 0.24773\ttraining's amex: 0.788517\tvalid_1's binary_logloss: 0.254957\tvalid_1's amex: 0.775052\n",
      "[2000]\ttraining's binary_logloss: 0.21364\ttraining's amex: 0.814003\tvalid_1's binary_logloss: 0.228886\tvalid_1's amex: 0.785646\n",
      "[3000]\ttraining's binary_logloss: 0.199275\ttraining's amex: 0.83222\tvalid_1's binary_logloss: 0.223007\tvalid_1's amex: 0.791603\n",
      "[4000]\ttraining's binary_logloss: 0.187013\ttraining's amex: 0.851372\tvalid_1's binary_logloss: 0.220358\tvalid_1's amex: 0.793425\n",
      "[5000]\ttraining's binary_logloss: 0.177694\ttraining's amex: 0.868223\tvalid_1's binary_logloss: 0.219398\tvalid_1's amex: 0.795038\n",
      "[6000]\ttraining's binary_logloss: 0.168414\ttraining's amex: 0.883994\tvalid_1's binary_logloss: 0.218594\tvalid_1's amex: 0.79563\n",
      "[7000]\ttraining's binary_logloss: 0.158788\ttraining's amex: 0.900569\tvalid_1's binary_logloss: 0.217811\tvalid_1's amex: 0.796445\n",
      "[8000]\ttraining's binary_logloss: 0.150939\ttraining's amex: 0.914422\tvalid_1's binary_logloss: 0.217536\tvalid_1's amex: 0.795839\n",
      "[9000]\ttraining's binary_logloss: 0.143114\ttraining's amex: 0.927376\tvalid_1's binary_logloss: 0.217194\tvalid_1's amex: 0.795768\n",
      "Our fold 1 CV score is 0.7955286609815184\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 2905 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 2\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.685991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 904407\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 2902\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[1000]\ttraining's binary_logloss: 0.248052\ttraining's amex: 0.789066\tvalid_1's binary_logloss: 0.254481\tvalid_1's amex: 0.775539\n",
      "[2000]\ttraining's binary_logloss: 0.214039\ttraining's amex: 0.813913\tvalid_1's binary_logloss: 0.227905\tvalid_1's amex: 0.788982\n",
      "[3000]\ttraining's binary_logloss: 0.199652\ttraining's amex: 0.832004\tvalid_1's binary_logloss: 0.221836\tvalid_1's amex: 0.792947\n",
      "[4000]\ttraining's binary_logloss: 0.187443\ttraining's amex: 0.850963\tvalid_1's binary_logloss: 0.21913\tvalid_1's amex: 0.796267\n",
      "[5000]\ttraining's binary_logloss: 0.178143\ttraining's amex: 0.86765\tvalid_1's binary_logloss: 0.218009\tvalid_1's amex: 0.797942\n",
      "[6000]\ttraining's binary_logloss: 0.168903\ttraining's amex: 0.883111\tvalid_1's binary_logloss: 0.217054\tvalid_1's amex: 0.797706\n",
      "[7000]\ttraining's binary_logloss: 0.159365\ttraining's amex: 0.899902\tvalid_1's binary_logloss: 0.216369\tvalid_1's amex: 0.797929\n",
      "[8000]\ttraining's binary_logloss: 0.151462\ttraining's amex: 0.913763\tvalid_1's binary_logloss: 0.215973\tvalid_1's amex: 0.797914\n",
      "Our fold 2 CV score is 0.7976754714480288\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 2905 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m lgb_valid \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(x_val, y_val, categorical_feature\u001b[38;5;241m=\u001b[39mcat_features)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart Training fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlgb_valid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlgb_amex_metric\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Save best model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(model, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/model_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_seed\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/amex/lib/python3.10/site-packages/lightgbm/engine.py:271\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[1;32m    273\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/amex/lib/python3.10/site-packages/lightgbm/basic.py:2605\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[1;32m   2598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_network(\n\u001b[1;32m   2599\u001b[0m         machines\u001b[38;5;241m=\u001b[39mmachines,\n\u001b[1;32m   2600\u001b[0m         local_listen_port\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_listen_port\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   2601\u001b[0m         listen_time_out\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m120\u001b[39m),\n\u001b[1;32m   2602\u001b[0m         num_machines\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_machines\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2603\u001b[0m     )\n\u001b[1;32m   2604\u001b[0m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[0;32m-> 2605\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[1;32m   2607\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n",
      "File \u001b[0;32m~/miniconda3/envs/amex/lib/python3.10/site-packages/lightgbm/basic.py:1815\u001b[0m, in \u001b[0;36mDataset.construct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1812\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_init_score_by_predictor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, used_indices)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[0;32m-> 1815\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1817\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1819\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_raw_data:\n\u001b[1;32m   1821\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/amex/lib/python3.10/site-packages/lightgbm/basic.py:1538\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__init_from_csc(data, params_str, ref_dataset)\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m-> 1538\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__init_from_np2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):\n",
      "File \u001b[0;32m~/miniconda3/envs/amex/lib/python3.10/site-packages/lightgbm/basic.py:1654\u001b[0m, in \u001b[0;36mDataset.__init_from_np2d\u001b[0;34m(self, mat, params_str, ref_dataset)\u001b[0m\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_void_p()\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mat\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32 \u001b[38;5;129;01mor\u001b[39;00m mat\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64:\n\u001b[0;32m-> 1654\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mmat\u001b[38;5;241m.\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# change non-float data to float data, need to copy\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(mat\u001b[38;5;241m.\u001b[39mreshape(mat\u001b[38;5;241m.\u001b[39msize), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_agg, target)):\n",
    "    n_estimator = n_est[fold]\n",
    "    if fold == 0:\n",
    "        continue\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {train_agg.shape[1]} features...')\n",
    "    print('-'*50)\n",
    "    x_train, x_val = train_agg.iloc[trn_ind], train_agg.iloc[val_ind]\n",
    "    y_train, y_val = target[trn_ind], target[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "    print(f\"Start Training fold {fold}\")\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = n_estimator,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 300,\n",
    "        verbose_eval = 1000,\n",
    "        feval = lgb_amex_metric\n",
    "    )\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'./models/model_fold{fold}_seed{seed}.pkl')\n",
    "    # Predict validation\n",
    "    y_val_pred = model.predict(x_val, raw_score=True)\n",
    "    val_score, val_g, val_t4 = amex_metric(y_val, y_val_pred)                                      \n",
    "    print(f'Our fold {fold} CV score is {val_score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74f3b7-a38d-4f47-aee8-fc0a0ac652d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6188ca-9b39-4438-af2a-19e7e7349b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "479e2ff2-8fe2-41ab-ae2d-8f1a33376ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 2273 features...\n",
      "--------------------------------------------------\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.267381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 484494\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 2273\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.247401\ttraining's amex: 0.784738\tvalid_1's binary_logloss: 0.256278\tvalid_1's amex: 0.771125\n",
      "[1000]\ttraining's binary_logloss: 0.219938\ttraining's amex: 0.804609\tvalid_1's binary_logloss: 0.23433\tvalid_1's amex: 0.781003\n",
      "[1500]\ttraining's binary_logloss: 0.206257\ttraining's amex: 0.821338\tvalid_1's binary_logloss: 0.2271\tvalid_1's amex: 0.788323\n",
      "[2000]\ttraining's binary_logloss: 0.197562\ttraining's amex: 0.834904\tvalid_1's binary_logloss: 0.224394\tvalid_1's amex: 0.790633\n",
      "[2500]\ttraining's binary_logloss: 0.19065\ttraining's amex: 0.84611\tvalid_1's binary_logloss: 0.222973\tvalid_1's amex: 0.792557\n",
      "[3000]\ttraining's binary_logloss: 0.182275\ttraining's amex: 0.858093\tvalid_1's binary_logloss: 0.221635\tvalid_1's amex: 0.793646\n",
      "[3500]\ttraining's binary_logloss: 0.17534\ttraining's amex: 0.870011\tvalid_1's binary_logloss: 0.221009\tvalid_1's amex: 0.793593\n",
      "[4000]\ttraining's binary_logloss: 0.167572\ttraining's amex: 0.88342\tvalid_1's binary_logloss: 0.220419\tvalid_1's amex: 0.792518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Booster.__del__ at 0x14f9de170>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wklee/miniconda3/envs/amex/lib/python3.10/site-packages/lightgbm/basic.py\", line 2664, in __del__\n",
      "    _safe_call(_LIB.LGBM_BoosterFree(self.handle))\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_agg, target)):\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {train_agg.shape[1]} features...')\n",
    "    print('-'*50)\n",
    "    x_train, x_val = train_agg.iloc[trn_ind], train_agg.iloc[val_ind]\n",
    "    y_train, y_val = target[trn_ind], target[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = 6000,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 300,\n",
    "        verbose_eval = 500,\n",
    "        feval = lgb_amex_metric\n",
    "    )\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'./models/model_fold{fold}_seed{seed}.pkl')\n",
    "    # Predict validation\n",
    "    y_val_pred = model.predict(x_val, raw_score=True)\n",
    "    # # Add to out of folds array\n",
    "    # oof_predictions[val_ind] = val_pred\n",
    "    # Predict the test set\n",
    "    # test_pred = model.predict(test[features])\n",
    "    # test_predictions += test_pred / CFG.n_folds\n",
    "    # Compute fold metric\n",
    "    val_score, val_g, val_t4 = amex_metric(y_val, y_val_pred)                                      \n",
    "    print(f'Our fold {fold} CV score is {val_score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04680e2b-df75-44f4-a2f1-6c2c0f90f004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

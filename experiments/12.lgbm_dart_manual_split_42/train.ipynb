{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd2ca34-3410-4078-aa48-7adbba211ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import joblib\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from collections import Counter\n",
    "from itertools import repeat\n",
    "from lightgbm import LGBMClassifier, log_evaluation\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a084bef6-4434-40fb-bafc-d7d3e4c57a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.common import (\n",
    "    sigmoid, pad_column_name\n",
    ")\n",
    "from utils.constants import *\n",
    "from utils.eval_helpers import (\n",
    "    plot_roc_curves, plot_feature_importance, \n",
    "    amex_metric, get_final_metric_df, amex_metric_np, lgb_amex_metric,\n",
    "    TreeExperiment\n",
    ")\n",
    "from utils.eda_helpers import (\n",
    "    plot_missing_proportion_barchart, \n",
    "    get_cols\n",
    ")\n",
    "from utils.extraction_helpers import read_file\n",
    "from utils.feature_group import (\n",
    "    CATEGORY_COLUMNS, CONTINUOUS_COLUMNS, NON_FEATURE_COLUMNS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67194602-57fc-4e92-9801-8d600e52a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885beac-e245-43b6-a2cc-73e9f871d132",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d46155-ea8e-4212-8c63-5b7ac8b08d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (458913, 5064)\n",
      "CPU times: user 17.4 s, sys: 22.5 s, total: 39.9 s\n",
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_agg = read_file(f\"../{PROCESSED_DATA_PATH}/v6/train_agg.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9c7632d-3cf9-4ff9-9467-9ebe9d2daecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (458913, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = read_file(f\"../{RAW_DATA_PATH}/train_labels.csv\")\n",
    "target = labels[\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46942b4a-fd5a-4bd0-9dbd-7ceb4b99f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# train_agg = train_agg.drop(columns=NON_FEATURE_COLUMNS + [\"target\"], errors=\"ignore\")\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe8f01a-341d-4cbe-8c46-8ae026830c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B_30_last', 'D_63_first', 'D_116_second_last', 'D_64_second_last', 'D_68_second_last', 'D_117_third_last', 'D_68_last', 'D_92_last', 'B_30_second_last', 'D_126_second_last', 'D_120_first', 'D_116_first', 'B_38_last', 'D_117_second_last', 'D_92_second_last', 'B_30_first', 'D_114_first', 'B_38_third_last', 'B_38_second_last', 'D_63_last', 'D_64_last', 'D_126_third_last', 'B_30_third_last', 'D_117_first', 'D_114_third_last', 'D_68_first', 'D_114_second_last', 'D_92_third_last', 'D_126_last', 'D_68_third_last', 'D_120_last', 'D_116_last', 'D_63_third_last', 'D_64_third_last', 'D_116_third_last', 'D_63_second_last', 'B_38_first', 'D_64_first', 'D_120_third_last', 'D_120_second_last', 'D_117_last', 'D_126_first', 'D_92_first', 'D_114_last']\n"
     ]
    }
   ],
   "source": [
    "cat_columns = get_cols(train_agg, CATEGORY_COLUMNS)\n",
    "cat_features = list(set(cat_columns).intersection(train_agg.columns))\n",
    "print(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b167a731-6108-48d0-85a6-42604a690325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_agg = pd.concat([train_agg, labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80925934-1a9e-4dda-b8dc-4557eb8c6d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((458913, 5064), (458913,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_agg.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "859b95fb-f92e-49f7-bf3e-b883059fc342",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_predict_group_df = pd.read_csv(f\"normal_predict_group.csv\").drop(columns=\"target\")\n",
    "default_predict_group_df = pd.read_csv(f\"default_predict_group.csv\").drop(columns=\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a9220a-b8e0-4e8b-a3a3-cbe8e82470e6",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23e515a9-789e-4f6c-bea4-a3a093aa8b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 499 ms, sys: 276 ms, total: 775 ms\n",
      "Wall time: 154 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lgbm_gbdt = TreeExperiment(\n",
    "    exp_full_path=\"../../experiments/11.lgbm_dart_round_clip_7788\",\n",
    "    seed=7788, \n",
    "    model_path=\"gbdt_models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9562b28-dc94-4a6e-ab64-4f2d02ef7e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = lgbm_gbdt.feature_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ea81ad8-8f06-4155-99e8-4798cb1a6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = []\n",
    "for i in range(5):\n",
    "    master.extend(fi.nsmallest(1167, f\"importance{i}\")[\"feature\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fafef0cb-181e-452d-8782-1534e7a1848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_dict = dict(Counter(master))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "577ff13b-d689-4a40-b6b2-517cdad9152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = [k for k, v in fi_dict.items() if v >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7087f58d-a7c2-4bb3-8365-bcdcada1f9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "564"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(col_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3cf6cdfc-dad0-49cc-8959-458075bfb784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458913, 5064)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6161818d-2842-4c11-be3d-4ba2341b5fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 427 ms, sys: 3.36 s, total: 3.79 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_agg = train_agg.drop(columns=col_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0dd0a24-a871-4456-a96c-a4e0d6a2afd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458913, 4500)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec4a4f97-25b8-407d-9711-57db7dc74ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B_30_last', 'D_63_first', 'D_64_second_last', 'D_68_second_last', 'D_117_third_last', 'D_68_last', 'D_92_last', 'B_30_second_last', 'D_126_second_last', 'D_120_first', 'B_38_last', 'D_117_second_last', 'D_92_second_last', 'D_114_first', 'B_38_third_last', 'D_63_last', 'B_38_second_last', 'D_64_last', 'D_126_third_last', 'B_30_third_last', 'D_117_first', 'D_114_third_last', 'D_68_first', 'D_114_second_last', 'D_126_last', 'D_68_third_last', 'D_120_last', 'D_63_third_last', 'D_64_third_last', 'D_63_second_last', 'B_38_first', 'D_64_first', 'D_120_third_last', 'D_120_second_last', 'D_117_last', 'D_126_first', 'D_92_first', 'D_114_last']\n"
     ]
    }
   ],
   "source": [
    "cat_columns = get_cols(train_agg, CATEGORY_COLUMNS)\n",
    "cat_features = list(set(cat_columns).intersection(train_agg.columns))\n",
    "print(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8d30d5ec-df43-41d9-b1c7-7b588ef7a711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a2a81-1e54-47cc-8193-c34571555192",
   "metadata": {},
   "source": [
    "### REAL Stratify Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b3ab433a-22a2-41a0-9115-b63d1b13222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.35 s, sys: 4.13 s, total: 5.48 s\n",
      "Wall time: 7.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "normal_train_agg = train_agg.loc[labels[\"target\"] == 0]\n",
    "default_train_agg = train_agg.loc[labels[\"target\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8c357c02-6cb5-48bb-aee0-86eb64543932",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_train_agg = normal_train_agg.merge(normal_predict_group_df, on=\"customer_ID\", how=\"left\")\n",
    "default_train_agg = default_train_agg.merge(default_predict_group_df, on=\"customer_ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "157d6345-4a81-4a99-8477-ad28b9a9d759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal_train_agg = normal_train_agg.drop(columns=get_cols(normal_train_agg, \"target\"))\n",
    "# default_train_agg = default_train_agg.drop(columns=get_cols(default_train_agg, \"target\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "39e45a16-b553-4c19-b70e-906e9edd543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f1ed9673-b449-4ccd-be0e-906327ee52ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f3ad0490-f83a-4e2c-a1ac-676db012b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_indices = {}\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(normal_train_agg, normal_train_agg[\"group\"])):\n",
    "    normal_indices[fold] = (trn_ind, val_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "06a8b6b2-0e67-4e37-9b61-b67b6bd29b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_indices = {}\n",
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(default_train_agg, default_train_agg[\"group\"])):\n",
    "    default_indices[fold] = (trn_ind, val_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "579b38b9-492e-4de7-9645-1a8a1eca7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp12_indices = {\"normal\": normal_indices, \"default\": default_indices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7ed6506d-c26c-4c13-8e4c-8bdfd9e89695",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_indices = {}\n",
    "for fold in range(5):\n",
    "    a = normal_train_agg.loc[normal_indices[fold][1], \"customer_ID\"].tolist()\n",
    "    b = default_train_agg.loc[default_indices[fold][1], \"customer_ID\"].tolist()\n",
    "    kfold_indices[fold] = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ddee0516-7c23-4fcc-a7b2-05f7bfe3e0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./5fold_val_cid.pkl']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(kfold_indices, \"./5fold_val_cid.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229f48a3-7ac4-475f-974a-0688c06597d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ce39854-1382-4213-a78b-99519a4512e3",
   "metadata": {},
   "source": [
    "### Hyperparams Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bcbcfe03-5c23-4997-960c-82ce41cf72d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'first_metric_only': True,\n",
    "    'metric': \"binary_logloss\",\n",
    "    'boosting': 'dart',\n",
    "    'device': \"cpu\",\n",
    "    'seed': seed,\n",
    "    'num_leaves': 96,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.195,\n",
    "    'bagging_freq': 10,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l2': 5,\n",
    "    'min_data_in_leaf': 120,\n",
    "    'scale_pos_weight': 1.28,\n",
    "    'max_bins': 255,\n",
    "    'feature_fraction_bynode': 0.9,\n",
    "    'drop_rate': 0.09,\n",
    "    'skip_drop': 0.55\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "29b170e8-eac9-4d09-970d-14244f215eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [8000, 8000, 8000, 8000, 8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5ca0e6e4-58f2-473d-9be4-4d8cfa0c4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ = train_.drop(columns=[\"customer_ID\", \"group\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b100e374-aeb9-4507-b66a-541d1e7b40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_x_columns = ['customer_ID', 'target', 'group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "75dd910d-e070-490e-ac5c-73c04f8724af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimator: 8000\n",
      "X_train: 367130\n",
      "X_val: 91783\n",
      "Y_train: 367130\n",
      "Y_validation: 91783\n",
      "Start Training fold 0\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.703449 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 566106\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 4487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.288424\ttraining's amex: 0.780779\tvalid_1's binary_logloss: 0.294466\tvalid_1's amex: 0.764587\n",
      "[1000]\ttraining's binary_logloss: 0.230785\ttraining's amex: 0.798886\tvalid_1's binary_logloss: 0.241922\tvalid_1's amex: 0.776767\n",
      "[1500]\ttraining's binary_logloss: 0.215633\ttraining's amex: 0.813358\tvalid_1's binary_logloss: 0.231175\tvalid_1's amex: 0.782011\n",
      "[2000]\ttraining's binary_logloss: 0.205129\ttraining's amex: 0.825006\tvalid_1's binary_logloss: 0.225635\tvalid_1's amex: 0.786633\n",
      "[2500]\ttraining's binary_logloss: 0.196034\ttraining's amex: 0.836092\tvalid_1's binary_logloss: 0.222343\tvalid_1's amex: 0.789767\n",
      "[3000]\ttraining's binary_logloss: 0.189434\ttraining's amex: 0.846889\tvalid_1's binary_logloss: 0.220763\tvalid_1's amex: 0.791844\n",
      "[3500]\ttraining's binary_logloss: 0.183562\ttraining's amex: 0.85644\tvalid_1's binary_logloss: 0.219678\tvalid_1's amex: 0.792377\n",
      "[4000]\ttraining's binary_logloss: 0.17693\ttraining's amex: 0.867098\tvalid_1's binary_logloss: 0.218661\tvalid_1's amex: 0.793084\n",
      "[4500]\ttraining's binary_logloss: 0.170523\ttraining's amex: 0.87816\tvalid_1's binary_logloss: 0.218018\tvalid_1's amex: 0.793203\n",
      "[5000]\ttraining's binary_logloss: 0.164532\ttraining's amex: 0.888038\tvalid_1's binary_logloss: 0.21752\tvalid_1's amex: 0.793619\n",
      "[5500]\ttraining's binary_logloss: 0.159177\ttraining's amex: 0.89797\tvalid_1's binary_logloss: 0.217219\tvalid_1's amex: 0.793986\n",
      "[6000]\ttraining's binary_logloss: 0.154154\ttraining's amex: 0.906545\tvalid_1's binary_logloss: 0.216974\tvalid_1's amex: 0.793853\n",
      "[6500]\ttraining's binary_logloss: 0.149578\ttraining's amex: 0.914598\tvalid_1's binary_logloss: 0.216826\tvalid_1's amex: 0.793837\n",
      "[7000]\ttraining's binary_logloss: 0.144991\ttraining's amex: 0.922007\tvalid_1's binary_logloss: 0.216681\tvalid_1's amex: 0.794142\n",
      "[7500]\ttraining's binary_logloss: 0.140451\ttraining's amex: 0.929356\tvalid_1's binary_logloss: 0.216555\tvalid_1's amex: 0.794326\n",
      "[8000]\ttraining's binary_logloss: 0.136295\ttraining's amex: 0.936329\tvalid_1's binary_logloss: 0.216519\tvalid_1's amex: 0.793915\n",
      "Our fold 0 CV score is 0.7936758241645094\n",
      "Number of estimator: 8000\n",
      "X_train: 367130\n",
      "X_val: 91783\n",
      "Y_train: 367130\n",
      "Y_validation: 91783\n",
      "Start Training fold 1\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.485619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 565984\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 4487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.288824\ttraining's amex: 0.779613\tvalid_1's binary_logloss: 0.29344\tvalid_1's amex: 0.767531\n",
      "[1000]\ttraining's binary_logloss: 0.231481\ttraining's amex: 0.797812\tvalid_1's binary_logloss: 0.240571\tvalid_1's amex: 0.778705\n",
      "[1500]\ttraining's binary_logloss: 0.216172\ttraining's amex: 0.812111\tvalid_1's binary_logloss: 0.229635\tvalid_1's amex: 0.785421\n",
      "[2000]\ttraining's binary_logloss: 0.205678\ttraining's amex: 0.823271\tvalid_1's binary_logloss: 0.224113\tvalid_1's amex: 0.790013\n",
      "[2500]\ttraining's binary_logloss: 0.196458\ttraining's amex: 0.834874\tvalid_1's binary_logloss: 0.220626\tvalid_1's amex: 0.792453\n",
      "[3000]\ttraining's binary_logloss: 0.189884\ttraining's amex: 0.845843\tvalid_1's binary_logloss: 0.21917\tvalid_1's amex: 0.794975\n",
      "[3500]\ttraining's binary_logloss: 0.184006\ttraining's amex: 0.855708\tvalid_1's binary_logloss: 0.218147\tvalid_1's amex: 0.796505\n",
      "[4000]\ttraining's binary_logloss: 0.177344\ttraining's amex: 0.86595\tvalid_1's binary_logloss: 0.217212\tvalid_1's amex: 0.797027\n",
      "[4500]\ttraining's binary_logloss: 0.171003\ttraining's amex: 0.876685\tvalid_1's binary_logloss: 0.216594\tvalid_1's amex: 0.797929\n",
      "[5000]\ttraining's binary_logloss: 0.164984\ttraining's amex: 0.887028\tvalid_1's binary_logloss: 0.216052\tvalid_1's amex: 0.798158\n",
      "[5500]\ttraining's binary_logloss: 0.159661\ttraining's amex: 0.896739\tvalid_1's binary_logloss: 0.215721\tvalid_1's amex: 0.798064\n",
      "[6000]\ttraining's binary_logloss: 0.154643\ttraining's amex: 0.905866\tvalid_1's binary_logloss: 0.215484\tvalid_1's amex: 0.798241\n",
      "[6500]\ttraining's binary_logloss: 0.150088\ttraining's amex: 0.913327\tvalid_1's binary_logloss: 0.215362\tvalid_1's amex: 0.798761\n",
      "[7000]\ttraining's binary_logloss: 0.145536\ttraining's amex: 0.920973\tvalid_1's binary_logloss: 0.21516\tvalid_1's amex: 0.799394\n",
      "[7500]\ttraining's binary_logloss: 0.140991\ttraining's amex: 0.92884\tvalid_1's binary_logloss: 0.214947\tvalid_1's amex: 0.799061\n",
      "[8000]\ttraining's binary_logloss: 0.136779\ttraining's amex: 0.935589\tvalid_1's binary_logloss: 0.214848\tvalid_1's amex: 0.799032\n",
      "Our fold 1 CV score is 0.798792526003361\n",
      "Number of estimator: 8000\n",
      "X_train: 367130\n",
      "X_val: 91783\n",
      "Y_train: 367130\n",
      "Y_validation: 91783\n",
      "Start Training fold 2\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.428364 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 565919\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 4487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.288769\ttraining's amex: 0.779353\tvalid_1's binary_logloss: 0.293421\tvalid_1's amex: 0.770597\n",
      "[1000]\ttraining's binary_logloss: 0.231303\ttraining's amex: 0.798596\tvalid_1's binary_logloss: 0.240635\tvalid_1's amex: 0.779752\n",
      "[1500]\ttraining's binary_logloss: 0.216024\ttraining's amex: 0.812684\tvalid_1's binary_logloss: 0.229758\tvalid_1's amex: 0.786159\n",
      "[2000]\ttraining's binary_logloss: 0.20557\ttraining's amex: 0.823841\tvalid_1's binary_logloss: 0.224131\tvalid_1's amex: 0.790192\n",
      "[2500]\ttraining's binary_logloss: 0.196318\ttraining's amex: 0.835157\tvalid_1's binary_logloss: 0.220603\tvalid_1's amex: 0.792965\n",
      "[3000]\ttraining's binary_logloss: 0.189781\ttraining's amex: 0.846274\tvalid_1's binary_logloss: 0.219259\tvalid_1's amex: 0.795361\n",
      "[3500]\ttraining's binary_logloss: 0.183888\ttraining's amex: 0.85631\tvalid_1's binary_logloss: 0.218257\tvalid_1's amex: 0.796537\n",
      "[4000]\ttraining's binary_logloss: 0.177254\ttraining's amex: 0.866475\tvalid_1's binary_logloss: 0.217238\tvalid_1's amex: 0.797197\n",
      "[4500]\ttraining's binary_logloss: 0.170862\ttraining's amex: 0.877031\tvalid_1's binary_logloss: 0.216614\tvalid_1's amex: 0.797272\n",
      "[5000]\ttraining's binary_logloss: 0.164863\ttraining's amex: 0.887716\tvalid_1's binary_logloss: 0.216083\tvalid_1's amex: 0.797239\n",
      "[5500]\ttraining's binary_logloss: 0.159517\ttraining's amex: 0.897257\tvalid_1's binary_logloss: 0.215756\tvalid_1's amex: 0.797465\n",
      "[6000]\ttraining's binary_logloss: 0.154473\ttraining's amex: 0.906135\tvalid_1's binary_logloss: 0.21557\tvalid_1's amex: 0.798195\n",
      "[6500]\ttraining's binary_logloss: 0.149914\ttraining's amex: 0.914346\tvalid_1's binary_logloss: 0.215369\tvalid_1's amex: 0.798789\n",
      "[7000]\ttraining's binary_logloss: 0.145341\ttraining's amex: 0.921413\tvalid_1's binary_logloss: 0.215249\tvalid_1's amex: 0.798572\n",
      "[7500]\ttraining's binary_logloss: 0.140808\ttraining's amex: 0.92858\tvalid_1's binary_logloss: 0.215016\tvalid_1's amex: 0.798842\n",
      "[8000]\ttraining's binary_logloss: 0.136612\ttraining's amex: 0.935406\tvalid_1's binary_logloss: 0.214889\tvalid_1's amex: 0.798599\n",
      "Our fold 2 CV score is 0.7983607073892527\n",
      "Number of estimator: 8000\n",
      "X_train: 367131\n",
      "X_val: 91782\n",
      "Y_train: 367131\n",
      "Y_validation: 91782\n",
      "Start Training fold 3\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.381134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 565929\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 4487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.288854\ttraining's amex: 0.779436\tvalid_1's binary_logloss: 0.292844\tvalid_1's amex: 0.769673\n",
      "[1000]\ttraining's binary_logloss: 0.231314\ttraining's amex: 0.798558\tvalid_1's binary_logloss: 0.239894\tvalid_1's amex: 0.783305\n",
      "[1500]\ttraining's binary_logloss: 0.216168\ttraining's amex: 0.812184\tvalid_1's binary_logloss: 0.229099\tvalid_1's amex: 0.787676\n",
      "[2000]\ttraining's binary_logloss: 0.205676\ttraining's amex: 0.823267\tvalid_1's binary_logloss: 0.223414\tvalid_1's amex: 0.791182\n",
      "[2500]\ttraining's binary_logloss: 0.19651\ttraining's amex: 0.83546\tvalid_1's binary_logloss: 0.219886\tvalid_1's amex: 0.793223\n",
      "[3000]\ttraining's binary_logloss: 0.189928\ttraining's amex: 0.845797\tvalid_1's binary_logloss: 0.218313\tvalid_1's amex: 0.795373\n",
      "[3500]\ttraining's binary_logloss: 0.184113\ttraining's amex: 0.855742\tvalid_1's binary_logloss: 0.217322\tvalid_1's amex: 0.796092\n",
      "[4000]\ttraining's binary_logloss: 0.177418\ttraining's amex: 0.865991\tvalid_1's binary_logloss: 0.216289\tvalid_1's amex: 0.798172\n",
      "[4500]\ttraining's binary_logloss: 0.170998\ttraining's amex: 0.877099\tvalid_1's binary_logloss: 0.215678\tvalid_1's amex: 0.797775\n",
      "[5000]\ttraining's binary_logloss: 0.165008\ttraining's amex: 0.887348\tvalid_1's binary_logloss: 0.215258\tvalid_1's amex: 0.798713\n",
      "[5500]\ttraining's binary_logloss: 0.159658\ttraining's amex: 0.897049\tvalid_1's binary_logloss: 0.214986\tvalid_1's amex: 0.79865\n",
      "[6000]\ttraining's binary_logloss: 0.154632\ttraining's amex: 0.906379\tvalid_1's binary_logloss: 0.214834\tvalid_1's amex: 0.798559\n",
      "[6500]\ttraining's binary_logloss: 0.15007\ttraining's amex: 0.913915\tvalid_1's binary_logloss: 0.214709\tvalid_1's amex: 0.798744\n",
      "[7000]\ttraining's binary_logloss: 0.145483\ttraining's amex: 0.921401\tvalid_1's binary_logloss: 0.214535\tvalid_1's amex: 0.798899\n",
      "[7500]\ttraining's binary_logloss: 0.140948\ttraining's amex: 0.928409\tvalid_1's binary_logloss: 0.214446\tvalid_1's amex: 0.798619\n",
      "[8000]\ttraining's binary_logloss: 0.136793\ttraining's amex: 0.935088\tvalid_1's binary_logloss: 0.214442\tvalid_1's amex: 0.798402\n",
      "Our fold 3 CV score is 0.7981916621227327\n",
      "Number of estimator: 8000\n",
      "X_train: 367131\n",
      "X_val: 91782\n",
      "Y_train: 367131\n",
      "Y_validation: 91782\n",
      "Start Training fold 4\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.344941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 565933\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 4487\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.288663\ttraining's amex: 0.77984\tvalid_1's binary_logloss: 0.293315\tvalid_1's amex: 0.767083\n",
      "[1000]\ttraining's binary_logloss: 0.231094\ttraining's amex: 0.798448\tvalid_1's binary_logloss: 0.240738\tvalid_1's amex: 0.779212\n",
      "[1500]\ttraining's binary_logloss: 0.215836\ttraining's amex: 0.812094\tvalid_1's binary_logloss: 0.229875\tvalid_1's amex: 0.785753\n",
      "[2000]\ttraining's binary_logloss: 0.205342\ttraining's amex: 0.82348\tvalid_1's binary_logloss: 0.224272\tvalid_1's amex: 0.78986\n",
      "[2500]\ttraining's binary_logloss: 0.1962\ttraining's amex: 0.83535\tvalid_1's binary_logloss: 0.220925\tvalid_1's amex: 0.792608\n",
      "[3000]\ttraining's binary_logloss: 0.189629\ttraining's amex: 0.845638\tvalid_1's binary_logloss: 0.219416\tvalid_1's amex: 0.79418\n",
      "[3500]\ttraining's binary_logloss: 0.183779\ttraining's amex: 0.855616\tvalid_1's binary_logloss: 0.21837\tvalid_1's amex: 0.795608\n",
      "[4000]\ttraining's binary_logloss: 0.177151\ttraining's amex: 0.865959\tvalid_1's binary_logloss: 0.217472\tvalid_1's amex: 0.796249\n",
      "[4500]\ttraining's binary_logloss: 0.170734\ttraining's amex: 0.877301\tvalid_1's binary_logloss: 0.216927\tvalid_1's amex: 0.796006\n",
      "[5000]\ttraining's binary_logloss: 0.164777\ttraining's amex: 0.887215\tvalid_1's binary_logloss: 0.216483\tvalid_1's amex: 0.795919\n",
      "[5500]\ttraining's binary_logloss: 0.159377\ttraining's amex: 0.897635\tvalid_1's binary_logloss: 0.21623\tvalid_1's amex: 0.796048\n",
      "[6000]\ttraining's binary_logloss: 0.154325\ttraining's amex: 0.906239\tvalid_1's binary_logloss: 0.216003\tvalid_1's amex: 0.795693\n",
      "[6500]\ttraining's binary_logloss: 0.149759\ttraining's amex: 0.914282\tvalid_1's binary_logloss: 0.215863\tvalid_1's amex: 0.795253\n",
      "[7000]\ttraining's binary_logloss: 0.145208\ttraining's amex: 0.921555\tvalid_1's binary_logloss: 0.215754\tvalid_1's amex: 0.795391\n",
      "[7500]\ttraining's binary_logloss: 0.140668\ttraining's amex: 0.929315\tvalid_1's binary_logloss: 0.215591\tvalid_1's amex: 0.795466\n",
      "[8000]\ttraining's binary_logloss: 0.136456\ttraining's amex: 0.935854\tvalid_1's binary_logloss: 0.215484\tvalid_1's amex: 0.795113\n",
      "Our fold 4 CV score is 0.7949018082701897\n"
     ]
    }
   ],
   "source": [
    "for fold in range(5):\n",
    "    \n",
    "    n_estimator = n_est[fold]\n",
    "    print(f\"Number of estimator: {n_estimator}\")\n",
    "    xn = normal_train_agg.loc[normal_indices[fold][0]].drop(columns=not_in_x_columns)\n",
    "    xd = default_train_agg.loc[default_indices[fold][0]].drop(columns=not_in_x_columns)\n",
    "    x_train = pd.concat([xn, xd], ignore_index=True)\n",
    "    print(f\"X_train: {x_train.shape[0]}\")\n",
    "    xn = normal_train_agg.loc[normal_indices[fold][1]].drop(columns=not_in_x_columns)\n",
    "    xd = default_train_agg.loc[default_indices[fold][1]].drop(columns=not_in_x_columns)\n",
    "    x_val = pd.concat([xn, xd], ignore_index=True)\n",
    "    print(f\"X_val: {x_val.shape[0]}\")\n",
    "    \n",
    "    yn = normal_train_agg.loc[normal_indices[fold][0], \"target\"]\n",
    "    yd = default_train_agg.loc[default_indices[fold][0], \"target\"]\n",
    "    y_train = pd.concat([yn, yd], ignore_index=True)\n",
    "    print(f\"Y_train: {y_train.shape[0]}\")\n",
    "    \n",
    "    yn = normal_train_agg.loc[normal_indices[fold][1], \"target\"]\n",
    "    yd = default_train_agg.loc[default_indices[fold][1], \"target\"]\n",
    "    y_val = pd.concat([yn, yd], ignore_index=True)\n",
    "    print(f\"Y_validation: {y_val.shape[0]}\")\n",
    "    \n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "    print(f\"Start Training fold {fold}\")\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = n_estimator,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 600,\n",
    "        verbose_eval = 500,\n",
    "        feval = lgb_amex_metric\n",
    "    )\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'./dart_models/model_fold{fold}_seed{seed}.pkl')\n",
    "    # Predict validation\n",
    "    y_val_pred = model.predict(x_val, raw_score=True)\n",
    "    val_score, val_g, val_t4 = amex_metric(y_val, y_val_pred)                                      \n",
    "    print(f'Our fold {fold} CV score is {val_score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735a0301-0cc4-4392-9a65-76856ed30744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 3983 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 3\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.258323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 553242\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 3983\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.265894\ttraining's amex: 0.784579\tvalid_1's binary_logloss: 0.270852\tvalid_1's amex: 0.776311\n",
      "[1000]\ttraining's binary_logloss: 0.220583\ttraining's amex: 0.804852\tvalid_1's binary_logloss: 0.231342\tvalid_1's amex: 0.786518\n",
      "[1500]\ttraining's binary_logloss: 0.205965\ttraining's amex: 0.821195\tvalid_1's binary_logloss: 0.22299\tvalid_1's amex: 0.793104\n",
      "[2000]\ttraining's binary_logloss: 0.197101\ttraining's amex: 0.835248\tvalid_1's binary_logloss: 0.220033\tvalid_1's amex: 0.795629\n",
      "[2500]\ttraining's binary_logloss: 0.187383\ttraining's amex: 0.848788\tvalid_1's binary_logloss: 0.217661\tvalid_1's amex: 0.796793\n"
     ]
    }
   ],
   "source": [
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_agg, target)):\n",
    "    if fold < 3:\n",
    "        continue\n",
    "    n_estimator = n_est[fold]\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {train_agg.shape[1]} features...')\n",
    "    print('-'*50)\n",
    "    x_train, x_val = train_agg.iloc[trn_ind], train_agg.iloc[val_ind]\n",
    "    y_train, y_val = target[trn_ind], target[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "    print(f\"Start Training fold {fold}\")\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = n_estimator,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 600,\n",
    "        verbose_eval = 500,\n",
    "        feval = lgb_amex_metric\n",
    "    )\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'./dart_models/model_fold{fold}_seed{seed}.pkl')\n",
    "    # Predict validation\n",
    "    y_val_pred = model.predict(x_val, raw_score=True)\n",
    "    val_score, val_g, val_t4 = amex_metric(y_val, y_val_pred)                                      \n",
    "    print(f'Our fold {fold} CV score is {val_score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04680e2b-df75-44f4-a2f1-6c2c0f90f004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dd2ca34-3410-4078-aa48-7adbba211ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import joblib\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import os\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from collections import Counter\n",
    "from itertools import repeat\n",
    "from lightgbm import LGBMClassifier, log_evaluation\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, fbeta_score, make_scorer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb17d64c-4cdb-4906-be84-1348484a4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import ListedColormap\n",
    "from cycler import cycler\n",
    "from IPython.display import display\n",
    "from colorama import Fore, Back, Style\n",
    "plt.rcParams['axes.facecolor'] = '#0057b8' # blue\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=['#ffd700'] +\n",
    "                                         plt.rcParams['axes.prop_cycle'].by_key()['color'][1:])\n",
    "plt.rcParams['text.color'] = 'w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a084bef6-4434-40fb-bafc-d7d3e4c57a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.common import (\n",
    "    sigmoid, pad_column_name\n",
    ")\n",
    "from utils.constants import *\n",
    "from utils.eval_helpers import (\n",
    "    plot_roc_curves, plot_feature_importance, \n",
    "    amex_metric, get_final_metric_df, amex_metric_np, lgb_amex_metric,\n",
    "    TreeExperiment\n",
    ")\n",
    "from utils.eda_helpers import (\n",
    "    plot_missing_proportion_barchart, \n",
    "    get_cols\n",
    ")\n",
    "from utils.extraction_helpers import read_file\n",
    "from utils.feature_group import (\n",
    "    CATEGORY_COLUMNS, CONTINUOUS_COLUMNS, NON_FEATURE_COLUMNS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67194602-57fc-4e92-9801-8d600e52a5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885beac-e245-43b6-a2cc-73e9f871d132",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d46155-ea8e-4212-8c63-5b7ac8b08d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (458913, 5064)\n",
      "CPU times: user 17.5 s, sys: 19 s, total: 36.5 s\n",
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_agg = read_file(f\"../{PROCESSED_DATA_PATH}/v6/train_agg.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c7632d-3cf9-4ff9-9467-9ebe9d2daecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (458913, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = read_file(f\"../{RAW_DATA_PATH}/train_labels.csv\")\n",
    "target = labels[\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46942b4a-fd5a-4bd0-9dbd-7ceb4b99f5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 444 ms, sys: 2.33 s, total: 2.78 s\n",
      "Wall time: 3.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_agg = train_agg.drop(columns=NON_FEATURE_COLUMNS + [\"target\"], errors=\"ignore\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe8f01a-341d-4cbe-8c46-8ae026830c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D_68_last', 'D_116_last', 'D_64_last', 'D_120_last', 'B_38_first', 'D_114_third_last', 'D_116_second_last', 'D_63_third_last', 'D_92_second_last', 'D_114_first', 'D_68_second_last', 'D_120_first', 'B_38_last', 'D_117_last', 'D_126_last', 'D_120_third_last', 'D_63_first', 'D_116_first', 'D_92_first', 'D_117_first', 'D_126_second_last', 'B_30_first', 'D_63_second_last', 'D_120_second_last', 'B_30_second_last', 'D_64_second_last', 'B_38_third_last', 'B_30_last', 'D_63_last', 'B_38_second_last', 'D_116_third_last', 'D_126_first', 'D_117_third_last', 'B_30_third_last', 'D_92_third_last', 'D_114_second_last', 'D_114_last', 'D_68_third_last', 'D_92_last', 'D_64_first', 'D_117_second_last', 'D_126_third_last', 'D_64_third_last', 'D_68_first']\n"
     ]
    }
   ],
   "source": [
    "cat_columns = get_cols(train_agg, CATEGORY_COLUMNS)\n",
    "cat_features = list(set(cat_columns).intersection(train_agg.columns))\n",
    "print(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80925934-1a9e-4dda-b8dc-4557eb8c6d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((458913, 5062), (458913,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_agg.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8bbf3f-617e-4704-ba09-61391c7dd954",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7788"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187a870-069f-4eef-9574-305dfc44e45f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Use short experiment to determine suitable features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14a0cbec-cf07-4cb2-8476-aa7d4791823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': \"binary_logloss\",\n",
    "    'boosting': 'gbdt',\n",
    "    'device': \"cpu\",\n",
    "    'seed': seed,\n",
    "    'num_leaves': 80,\n",
    "    'learning_rate': 0.02,\n",
    "    'feature_fraction': 0.20,\n",
    "    'bagging_freq': 10,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l2': 10,\n",
    "    'min_data_in_leaf': 100,\n",
    "    'scale_pos_weight': 1.3,\n",
    "    'max_bins': 127\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4389b771-5189-4c02-8afe-bd3a90b51a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f8e55cf-5de5-41d2-9bb5-84bb7c516206",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [1800] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4531e9a3-9be8-4f2b-b07d-c489a4037de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 5062 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 0\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.449022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 350388\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 5022\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's binary_logloss: 0.256362\ttraining's amex: 0.775292\tvalid_1's binary_logloss: 0.262517\tvalid_1's amex: 0.762277\n",
      "[200]\ttraining's binary_logloss: 0.222455\ttraining's amex: 0.794466\tvalid_1's binary_logloss: 0.233136\tvalid_1's amex: 0.775488\n",
      "[300]\ttraining's binary_logloss: 0.210922\ttraining's amex: 0.808291\tvalid_1's binary_logloss: 0.226372\tvalid_1's amex: 0.783133\n",
      "[400]\ttraining's binary_logloss: 0.203243\ttraining's amex: 0.819148\tvalid_1's binary_logloss: 0.223623\tvalid_1's amex: 0.78565\n",
      "[500]\ttraining's binary_logloss: 0.197047\ttraining's amex: 0.828894\tvalid_1's binary_logloss: 0.222197\tvalid_1's amex: 0.787921\n",
      "[600]\ttraining's binary_logloss: 0.191487\ttraining's amex: 0.83803\tvalid_1's binary_logloss: 0.221259\tvalid_1's amex: 0.788634\n",
      "[700]\ttraining's binary_logloss: 0.186401\ttraining's amex: 0.846055\tvalid_1's binary_logloss: 0.220618\tvalid_1's amex: 0.789041\n",
      "[800]\ttraining's binary_logloss: 0.181556\ttraining's amex: 0.853722\tvalid_1's binary_logloss: 0.220055\tvalid_1's amex: 0.789227\n",
      "[900]\ttraining's binary_logloss: 0.176992\ttraining's amex: 0.861771\tvalid_1's binary_logloss: 0.219741\tvalid_1's amex: 0.790158\n",
      "[1000]\ttraining's binary_logloss: 0.172628\ttraining's amex: 0.869106\tvalid_1's binary_logloss: 0.219444\tvalid_1's amex: 0.790884\n",
      "[1100]\ttraining's binary_logloss: 0.168489\ttraining's amex: 0.876369\tvalid_1's binary_logloss: 0.21934\tvalid_1's amex: 0.790405\n",
      "[1200]\ttraining's binary_logloss: 0.164567\ttraining's amex: 0.883213\tvalid_1's binary_logloss: 0.219186\tvalid_1's amex: 0.790194\n",
      "Early stopping, best iteration is:\n",
      "[1029]\ttraining's binary_logloss: 0.171396\ttraining's amex: 0.871426\tvalid_1's binary_logloss: 0.219388\tvalid_1's amex: 0.79092\n",
      "Our fold 0 CV score is 0.7906806316039652\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 5062 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 1\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.544710 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 350266\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 5025\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's binary_logloss: 0.256814\ttraining's amex: 0.773972\tvalid_1's binary_logloss: 0.261046\tvalid_1's amex: 0.767493\n",
      "[200]\ttraining's binary_logloss: 0.223067\ttraining's amex: 0.79283\tvalid_1's binary_logloss: 0.231734\tvalid_1's amex: 0.781917\n",
      "[300]\ttraining's binary_logloss: 0.211373\ttraining's amex: 0.807099\tvalid_1's binary_logloss: 0.224837\tvalid_1's amex: 0.78938\n",
      "[400]\ttraining's binary_logloss: 0.203776\ttraining's amex: 0.817662\tvalid_1's binary_logloss: 0.222114\tvalid_1's amex: 0.792635\n",
      "[500]\ttraining's binary_logloss: 0.197538\ttraining's amex: 0.827556\tvalid_1's binary_logloss: 0.220678\tvalid_1's amex: 0.7943\n",
      "[600]\ttraining's binary_logloss: 0.191948\ttraining's amex: 0.83658\tvalid_1's binary_logloss: 0.219688\tvalid_1's amex: 0.795058\n",
      "[700]\ttraining's binary_logloss: 0.186894\ttraining's amex: 0.845317\tvalid_1's binary_logloss: 0.219142\tvalid_1's amex: 0.79571\n",
      "[800]\ttraining's binary_logloss: 0.182013\ttraining's amex: 0.853475\tvalid_1's binary_logloss: 0.218599\tvalid_1's amex: 0.79664\n",
      "[900]\ttraining's binary_logloss: 0.177442\ttraining's amex: 0.860983\tvalid_1's binary_logloss: 0.218218\tvalid_1's amex: 0.797655\n",
      "[1000]\ttraining's binary_logloss: 0.173041\ttraining's amex: 0.868218\tvalid_1's binary_logloss: 0.217885\tvalid_1's amex: 0.7986\n",
      "[1100]\ttraining's binary_logloss: 0.16886\ttraining's amex: 0.876048\tvalid_1's binary_logloss: 0.217662\tvalid_1's amex: 0.798195\n",
      "Early stopping, best iteration is:\n",
      "[967]\ttraining's binary_logloss: 0.174438\ttraining's amex: 0.865806\tvalid_1's binary_logloss: 0.217956\tvalid_1's amex: 0.799102\n",
      "Our fold 1 CV score is 0.7988837317427625\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 5062 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 2\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.345007 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 350275\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 5027\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's binary_logloss: 0.256681\ttraining's amex: 0.775078\tvalid_1's binary_logloss: 0.261385\tvalid_1's amex: 0.76501\n",
      "[200]\ttraining's binary_logloss: 0.222954\ttraining's amex: 0.794478\tvalid_1's binary_logloss: 0.231905\tvalid_1's amex: 0.777114\n",
      "[300]\ttraining's binary_logloss: 0.211391\ttraining's amex: 0.808235\tvalid_1's binary_logloss: 0.224839\tvalid_1's amex: 0.784177\n",
      "[400]\ttraining's binary_logloss: 0.20383\ttraining's amex: 0.818861\tvalid_1's binary_logloss: 0.221832\tvalid_1's amex: 0.788366\n",
      "[500]\ttraining's binary_logloss: 0.197648\ttraining's amex: 0.827725\tvalid_1's binary_logloss: 0.22033\tvalid_1's amex: 0.790353\n",
      "[600]\ttraining's binary_logloss: 0.192177\ttraining's amex: 0.836654\tvalid_1's binary_logloss: 0.219375\tvalid_1's amex: 0.790894\n",
      "[700]\ttraining's binary_logloss: 0.187093\ttraining's amex: 0.844947\tvalid_1's binary_logloss: 0.218756\tvalid_1's amex: 0.792118\n",
      "[800]\ttraining's binary_logloss: 0.182248\ttraining's amex: 0.852769\tvalid_1's binary_logloss: 0.218221\tvalid_1's amex: 0.793347\n",
      "[900]\ttraining's binary_logloss: 0.177654\ttraining's amex: 0.860877\tvalid_1's binary_logloss: 0.217876\tvalid_1's amex: 0.793934\n",
      "[1000]\ttraining's binary_logloss: 0.173341\ttraining's amex: 0.868585\tvalid_1's binary_logloss: 0.217626\tvalid_1's amex: 0.794379\n",
      "[1100]\ttraining's binary_logloss: 0.169116\ttraining's amex: 0.875698\tvalid_1's binary_logloss: 0.217368\tvalid_1's amex: 0.793912\n",
      "[1200]\ttraining's binary_logloss: 0.165108\ttraining's amex: 0.882631\tvalid_1's binary_logloss: 0.217138\tvalid_1's amex: 0.793943\n",
      "Early stopping, best iteration is:\n",
      "[1003]\ttraining's binary_logloss: 0.173209\ttraining's amex: 0.868828\tvalid_1's binary_logloss: 0.217624\tvalid_1's amex: 0.794948\n",
      "Our fold 2 CV score is 0.7947087193599699\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 5062 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 3\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.398807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 350308\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 5024\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's binary_logloss: 0.256958\ttraining's amex: 0.773963\tvalid_1's binary_logloss: 0.260571\tvalid_1's amex: 0.769823\n",
      "[200]\ttraining's binary_logloss: 0.223275\ttraining's amex: 0.792977\tvalid_1's binary_logloss: 0.230928\tvalid_1's amex: 0.783218\n",
      "[300]\ttraining's binary_logloss: 0.211719\ttraining's amex: 0.807603\tvalid_1's binary_logloss: 0.223959\tvalid_1's amex: 0.789748\n",
      "[400]\ttraining's binary_logloss: 0.204074\ttraining's amex: 0.818601\tvalid_1's binary_logloss: 0.220934\tvalid_1's amex: 0.792289\n",
      "[500]\ttraining's binary_logloss: 0.197873\ttraining's amex: 0.827585\tvalid_1's binary_logloss: 0.219389\tvalid_1's amex: 0.793877\n",
      "[600]\ttraining's binary_logloss: 0.192354\ttraining's amex: 0.836124\tvalid_1's binary_logloss: 0.218559\tvalid_1's amex: 0.794911\n",
      "[700]\ttraining's binary_logloss: 0.187094\ttraining's amex: 0.844713\tvalid_1's binary_logloss: 0.21784\tvalid_1's amex: 0.795712\n",
      "[800]\ttraining's binary_logloss: 0.182363\ttraining's amex: 0.852413\tvalid_1's binary_logloss: 0.217391\tvalid_1's amex: 0.796256\n",
      "[900]\ttraining's binary_logloss: 0.177807\ttraining's amex: 0.859981\tvalid_1's binary_logloss: 0.217042\tvalid_1's amex: 0.796067\n",
      "[1000]\ttraining's binary_logloss: 0.1734\ttraining's amex: 0.867564\tvalid_1's binary_logloss: 0.216747\tvalid_1's amex: 0.795649\n",
      "Early stopping, best iteration is:\n",
      "[880]\ttraining's binary_logloss: 0.178668\ttraining's amex: 0.858593\tvalid_1's binary_logloss: 0.217063\tvalid_1's amex: 0.796773\n",
      "Our fold 3 CV score is 0.7966667185921243\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 5062 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 4\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.640587 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 350249\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 5025\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's binary_logloss: 0.256408\ttraining's amex: 0.774285\tvalid_1's binary_logloss: 0.261545\tvalid_1's amex: 0.763166\n",
      "[200]\ttraining's binary_logloss: 0.222417\ttraining's amex: 0.794263\tvalid_1's binary_logloss: 0.232243\tvalid_1's amex: 0.77611\n",
      "[300]\ttraining's binary_logloss: 0.210984\ttraining's amex: 0.808391\tvalid_1's binary_logloss: 0.225703\tvalid_1's amex: 0.78266\n",
      "[400]\ttraining's binary_logloss: 0.203354\ttraining's amex: 0.819358\tvalid_1's binary_logloss: 0.223093\tvalid_1's amex: 0.786907\n",
      "[500]\ttraining's binary_logloss: 0.197115\ttraining's amex: 0.828993\tvalid_1's binary_logloss: 0.221655\tvalid_1's amex: 0.790374\n",
      "[600]\ttraining's binary_logloss: 0.191548\ttraining's amex: 0.837876\tvalid_1's binary_logloss: 0.220788\tvalid_1's amex: 0.790658\n",
      "[700]\ttraining's binary_logloss: 0.186385\ttraining's amex: 0.845549\tvalid_1's binary_logloss: 0.220095\tvalid_1's amex: 0.791202\n",
      "[800]\ttraining's binary_logloss: 0.181596\ttraining's amex: 0.853916\tvalid_1's binary_logloss: 0.219706\tvalid_1's amex: 0.792104\n",
      "[900]\ttraining's binary_logloss: 0.177069\ttraining's amex: 0.8617\tvalid_1's binary_logloss: 0.219387\tvalid_1's amex: 0.792536\n",
      "[1000]\ttraining's binary_logloss: 0.172721\ttraining's amex: 0.869098\tvalid_1's binary_logloss: 0.219071\tvalid_1's amex: 0.792719\n",
      "[1100]\ttraining's binary_logloss: 0.168553\ttraining's amex: 0.876074\tvalid_1's binary_logloss: 0.218969\tvalid_1's amex: 0.793489\n",
      "[1200]\ttraining's binary_logloss: 0.164577\ttraining's amex: 0.882804\tvalid_1's binary_logloss: 0.218828\tvalid_1's amex: 0.793383\n",
      "Early stopping, best iteration is:\n",
      "[1084]\ttraining's binary_logloss: 0.169215\ttraining's amex: 0.87523\tvalid_1's binary_logloss: 0.218963\tvalid_1's amex: 0.793851\n",
      "Our fold 4 CV score is 0.7936396367534907\n"
     ]
    }
   ],
   "source": [
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_agg, target)):\n",
    "    n_estimator = n_est[fold]\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {train_agg.shape[1]} features...')\n",
    "    print('-'*50)\n",
    "    x_train, x_val = train_agg.iloc[trn_ind], train_agg.iloc[val_ind]\n",
    "    y_train, y_val = target[trn_ind], target[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "    print(f\"Start Training fold {fold}\")\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = n_estimator,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 200,\n",
    "        verbose_eval = 100,\n",
    "        feval = lgb_amex_metric\n",
    "    )\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'./gbdt_models/model_fold{fold}_seed{seed}.pkl')\n",
    "    # Predict validation\n",
    "    y_val_pred = model.predict(x_val, raw_score=True)\n",
    "    val_score, val_g, val_t4 = amex_metric(y_val, y_val_pred)                                      \n",
    "    print(f'Our fold {fold} CV score is {val_score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171ab5b8-61d4-4bc9-889e-cdcdb9c7841a",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23e515a9-789e-4f6c-bea4-a3a093aa8b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 432 ms, sys: 204 ms, total: 636 ms\n",
      "Wall time: 128 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lgbm_gbdt = TreeExperiment(\n",
    "    exp_full_path=\"../../experiments/11.lgbm_dart_round_clip_7788\",\n",
    "    seed=7788, \n",
    "    model_path=\"gbdt_models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9562b28-dc94-4a6e-ab64-4f2d02ef7e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = lgbm_gbdt.feature_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4ea81ad8-8f06-4155-99e8-4798cb1a6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = []\n",
    "for i in range(5):\n",
    "    master.extend(fi.nsmallest(1700, f\"importance{i}\")[\"feature\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fafef0cb-181e-452d-8782-1534e7a1848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_dict = dict(Counter(master))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "577ff13b-d689-4a40-b6b2-517cdad9152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = [k for k, v in fi_dict.items() if v >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7087f58d-a7c2-4bb3-8365-bcdcada1f9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1079"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(col_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3cf6cdfc-dad0-49cc-8959-458075bfb784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458913, 5062)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6161818d-2842-4c11-be3d-4ba2341b5fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 403 ms, sys: 3.14 s, total: 3.54 s\n",
      "Wall time: 8.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_agg = train_agg.drop(columns=col_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d0dd0a24-a871-4456-a96c-a4e0d6a2afd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(458913, 3983)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "709c3164-79d5-4808-babd-a4442c1873e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ec4a4f97-25b8-407d-9711-57db7dc74ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D_68_last', 'D_64_last', 'D_120_last', 'B_38_first', 'D_114_third_last', 'D_63_third_last', 'D_114_first', 'D_68_second_last', 'D_120_first', 'B_38_last', 'D_117_last', 'D_126_last', 'D_120_third_last', 'D_63_first', 'D_92_first', 'D_117_first', 'D_126_second_last', 'D_63_second_last', 'D_120_second_last', 'B_30_second_last', 'D_64_second_last', 'B_38_third_last', 'B_30_last', 'D_63_last', 'B_38_second_last', 'D_126_first', 'D_117_third_last', 'B_30_third_last', 'D_114_second_last', 'D_114_last', 'D_68_third_last', 'D_64_first', 'D_117_second_last', 'D_126_third_last', 'D_64_third_last', 'D_68_first']\n"
     ]
    }
   ],
   "source": [
    "cat_columns = get_cols(train_agg, CATEGORY_COLUMNS)\n",
    "cat_features = list(set(cat_columns).intersection(train_agg.columns))\n",
    "print(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8d30d5ec-df43-41d9-b1c7-7b588ef7a711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63209e65-9d82-4b2e-bf34-f4a15560f08f",
   "metadata": {},
   "source": [
    "### Train LGBM using pre-set hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b4242c36-4936-45c9-a8e7-33ff9a312586",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'first_metric_only': True,\n",
    "    'metric': \"binary_logloss\",\n",
    "    'boosting': 'dart',\n",
    "    'device': \"cpu\",\n",
    "    'seed': seed,\n",
    "    'num_leaves': 93,\n",
    "    'learning_rate': 0.0115,\n",
    "    'feature_fraction': 0.195,\n",
    "    'bagging_freq': 10,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l2': 5,\n",
    "    'min_data_in_leaf': 110,\n",
    "    'scale_pos_weight': 1.3,\n",
    "    'max_bins': 255,\n",
    "    'feature_fraction_bynode': 0.9,\n",
    "    'drop_rate': 0.075,\n",
    "    'skip_drop': 0.6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c57a3a3d-a51c-436e-9b0b-d19a66668b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "29b170e8-eac9-4d09-970d-14244f215eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_est = [6000, 6500, 5500, 6000, 5500]\n",
    "# n_est = [9500] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "167f10c3-13e5-4a85-b3bb-479bc248f148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2431"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735a0301-0cc4-4392-9a65-76856ed30744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 3983 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 4\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.054606 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 553010\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 3983\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.262141\ttraining's amex: 0.786064\tvalid_1's binary_logloss: 0.26857\tvalid_1's amex: 0.769614\n"
     ]
    }
   ],
   "source": [
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_agg, target)):\n",
    "    if fold < 4:\n",
    "        continue\n",
    "    n_estimator = n_est[fold]\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {train_agg.shape[1]} features...')\n",
    "    print('-'*50)\n",
    "    x_train, x_val = train_agg.iloc[trn_ind], train_agg.iloc[val_ind]\n",
    "    y_train, y_val = target[trn_ind], target[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "    print(f\"Start Training fold {fold}\")\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = n_estimator,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 600,\n",
    "        verbose_eval = 500,\n",
    "        feval = lgb_amex_metric\n",
    "    )\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'./dart_models/model_fold{fold}_seed{seed}.pkl')\n",
    "    # Predict validation\n",
    "    y_val_pred = model.predict(x_val, raw_score=True)\n",
    "    val_score, val_g, val_t4 = amex_metric(y_val, y_val_pred)                                      \n",
    "    print(f'Our fold {fold} CV score is {val_score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "789eb69b-371a-412f-b6d8-63b7432e0382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 3983 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 0\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.156284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 553126\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 3983\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.38076\ttraining's amex: 0.769573\tvalid_1's binary_logloss: 0.384457\tvalid_1's amex: 0.756352\n",
      "[1000]\ttraining's binary_logloss: 0.278647\ttraining's amex: 0.783586\tvalid_1's binary_logloss: 0.286052\tvalid_1's amex: 0.766755\n",
      "[1500]\ttraining's binary_logloss: 0.242993\ttraining's amex: 0.797213\tvalid_1's binary_logloss: 0.254021\tvalid_1's amex: 0.774915\n",
      "[2000]\ttraining's binary_logloss: 0.226588\ttraining's amex: 0.807646\tvalid_1's binary_logloss: 0.241236\tvalid_1's amex: 0.779911\n",
      "[2500]\ttraining's binary_logloss: 0.21665\ttraining's amex: 0.816636\tvalid_1's binary_logloss: 0.235014\tvalid_1's amex: 0.783715\n",
      "[3000]\ttraining's binary_logloss: 0.209606\ttraining's amex: 0.824941\tvalid_1's binary_logloss: 0.231679\tvalid_1's amex: 0.785374\n",
      "[3500]\ttraining's binary_logloss: 0.202889\ttraining's amex: 0.83355\tvalid_1's binary_logloss: 0.229037\tvalid_1's amex: 0.787259\n",
      "[4000]\ttraining's binary_logloss: 0.196906\ttraining's amex: 0.841759\tvalid_1's binary_logloss: 0.227212\tvalid_1's amex: 0.788525\n",
      "[4500]\ttraining's binary_logloss: 0.189787\ttraining's amex: 0.851611\tvalid_1's binary_logloss: 0.225381\tvalid_1's amex: 0.790154\n",
      "[5000]\ttraining's binary_logloss: 0.184803\ttraining's amex: 0.860037\tvalid_1's binary_logloss: 0.224547\tvalid_1's amex: 0.790255\n",
      "[5500]\ttraining's binary_logloss: 0.179661\ttraining's amex: 0.868896\tvalid_1's binary_logloss: 0.22382\tvalid_1's amex: 0.791012\n",
      "[6000]\ttraining's binary_logloss: 0.17537\ttraining's amex: 0.876613\tvalid_1's binary_logloss: 0.223346\tvalid_1's amex: 0.791566\n",
      "[6500]\ttraining's binary_logloss: 0.170638\ttraining's amex: 0.884636\tvalid_1's binary_logloss: 0.222829\tvalid_1's amex: 0.791348\n",
      "[7000]\ttraining's binary_logloss: 0.166389\ttraining's amex: 0.892143\tvalid_1's binary_logloss: 0.222378\tvalid_1's amex: 0.791644\n",
      "[7500]\ttraining's binary_logloss: 0.162637\ttraining's amex: 0.899066\tvalid_1's binary_logloss: 0.222031\tvalid_1's amex: 0.791729\n",
      "[8000]\ttraining's binary_logloss: 0.158553\ttraining's amex: 0.905692\tvalid_1's binary_logloss: 0.221694\tvalid_1's amex: 0.792209\n",
      "[8500]\ttraining's binary_logloss: 0.154847\ttraining's amex: 0.91264\tvalid_1's binary_logloss: 0.221402\tvalid_1's amex: 0.792224\n",
      "[9000]\ttraining's binary_logloss: 0.151206\ttraining's amex: 0.918617\tvalid_1's binary_logloss: 0.221126\tvalid_1's amex: 0.79251\n",
      "[9500]\ttraining's binary_logloss: 0.147608\ttraining's amex: 0.924386\tvalid_1's binary_logloss: 0.220968\tvalid_1's amex: 0.792693\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/model_fold0_seed7788.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [96]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m     13\u001b[0m     params \u001b[38;5;241m=\u001b[39m params,\n\u001b[1;32m     14\u001b[0m     train_set \u001b[38;5;241m=\u001b[39m lgb_train,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     feval \u001b[38;5;241m=\u001b[39m lgb_amex_metric\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Save best model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./models/model_fold\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfold\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_seed\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mseed\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Predict validation\u001b[39;00m\n\u001b[1;32m     24\u001b[0m y_val_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_val, raw_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/amex/lib/python3.10/site-packages/joblib/numpy_pickle.py:481\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[1;32m    479\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[0;32m--> 481\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    482\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/model_fold0_seed7788.pkl'"
     ]
    }
   ],
   "source": [
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_agg, target)):\n",
    "    if fold == 0:\n",
    "        continue\n",
    "    n_estimator = n_est[fold]\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {train_agg.shape[1]} features...')\n",
    "    print('-'*50)\n",
    "    x_train, x_val = train_agg.iloc[trn_ind], train_agg.iloc[val_ind]\n",
    "    y_train, y_val = target[trn_ind], target[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "    print(f\"Start Training fold {fold}\")\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = n_estimator,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 600,\n",
    "        verbose_eval = 500,\n",
    "        feval = lgb_amex_metric\n",
    "    )\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'./models/model_fold{fold}_seed{seed}.pkl')\n",
    "    # Predict validation\n",
    "    y_val_pred = model.predict(x_val, raw_score=True)\n",
    "    val_score, val_g, val_t4 = amex_metric(y_val, y_val_pred)                                      \n",
    "    print(f'Our fold {fold} CV score is {val_score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491b4e8-78bc-4f47-bb8d-2fac68019bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_agg, target)):\n",
    "    n_estimator = n_est[fold]\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {train_agg.shape[1]} features...')\n",
    "    print('-'*50)\n",
    "    x_train, x_val = train_agg.iloc[trn_ind], train_agg.iloc[val_ind]\n",
    "    y_train, y_val = target[trn_ind], target[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "    print(f\"Start Training fold {fold}\")\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = n_estimator,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 300,\n",
    "        verbose_eval = 500,\n",
    "        feval = lgb_amex_metric\n",
    "    )\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'./models/model_fold{fold}_seed{seed}.pkl')\n",
    "    # Predict validation\n",
    "    y_val_pred = model.predict(x_val, raw_score=True)\n",
    "    val_score, val_g, val_t4 = amex_metric(y_val, y_val_pred)                                      \n",
    "    print(f'Our fold {fold} CV score is {val_score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04680e2b-df75-44f4-a2f1-6c2c0f90f004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

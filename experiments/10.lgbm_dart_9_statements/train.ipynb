{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd2ca34-3410-4078-aa48-7adbba211ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gc\n",
    "import joblib\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import os\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from itertools import repeat\n",
    "from lightgbm import LGBMClassifier, log_evaluation\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, fbeta_score, make_scorer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb17d64c-4cdb-4906-be84-1348484a4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.colors import ListedColormap\n",
    "from cycler import cycler\n",
    "from IPython.display import display\n",
    "from colorama import Fore, Back, Style\n",
    "plt.rcParams['axes.facecolor'] = '#0057b8' # blue\n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=['#ffd700'] +\n",
    "                                         plt.rcParams['axes.prop_cycle'].by_key()['color'][1:])\n",
    "plt.rcParams['text.color'] = 'w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a084bef6-4434-40fb-bafc-d7d3e4c57a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.common import (\n",
    "    sigmoid, pad_column_name\n",
    ")\n",
    "from utils.constants import *\n",
    "from utils.eval_helpers import (\n",
    "    plot_roc_curves, plot_feature_importance, \n",
    "    amex_metric, get_final_metric_df, amex_metric_np, lgb_amex_metric\n",
    ")\n",
    "from utils.eda_helpers import (\n",
    "    plot_missing_proportion_barchart, \n",
    "    get_cols\n",
    ")\n",
    "from utils.extraction_helpers import read_file\n",
    "from utils.feature_group import (\n",
    "    CATEGORY_COLUMNS, CONTINUOUS_COLUMNS, NON_FEATURE_COLUMNS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67194602-57fc-4e92-9801-8d600e52a5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885beac-e245-43b6-a2cc-73e9f871d132",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d46155-ea8e-4212-8c63-5b7ac8b08d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (458913, 4015)\n",
      "CPU times: user 13.7 s, sys: 12.9 s, total: 26.6 s\n",
      "Wall time: 14.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_agg = read_file(f\"../{PROCESSED_DATA_PATH}/v5/final_train_agg_v5.5.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9c7632d-3cf9-4ff9-9467-9ebe9d2daecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (458913, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = read_file(f\"../{RAW_DATA_PATH}/train_labels.csv\")\n",
    "target = labels[\"target\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46942b4a-fd5a-4bd0-9dbd-7ceb4b99f5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 381 ms, sys: 2.16 s, total: 2.54 s\n",
      "Wall time: 3.98 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_agg = train_agg.drop(columns=NON_FEATURE_COLUMNS + [\"target\"], errors=\"ignore\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe8f01a-341d-4cbe-8c46-8ae026830c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D_68_third_last', 'D_120_first', 'D_117_first', 'D_92_first', 'D_68_second_last', 'D_63_second_last', 'D_116_second_last', 'D_63_first', 'B_30_last', 'D_126_first', 'D_120_last', 'D_64_second_last', 'B_38_third_last', 'D_117_second_last', 'D_68_first', 'D_64_last', 'B_38_last', 'D_64_first', 'D_92_third_last', 'D_120_third_last', 'B_38_first', 'D_114_first', 'D_63_last', 'D_114_second_last', 'B_30_third_last', 'D_68_last', 'D_117_third_last', 'D_92_last', 'D_120_second_last', 'D_116_third_last', 'D_64_third_last', 'D_116_last', 'D_117_last', 'D_116_first', 'B_38_second_last', 'D_126_third_last', 'D_63_third_last', 'D_114_third_last', 'D_114_last', 'D_92_second_last']\n"
     ]
    }
   ],
   "source": [
    "cat_columns = get_cols(train_agg, CATEGORY_COLUMNS)\n",
    "cat_features = list(set(cat_columns).intersection(train_agg.columns))\n",
    "print(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80925934-1a9e-4dda-b8dc-4557eb8c6d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((458913, 4014), (458913,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_agg.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63209e65-9d82-4b2e-bf34-f4a15560f08f",
   "metadata": {},
   "source": [
    "### Train LGBM using pre-set hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3d2e782-8f4d-4e59-a03e-e962e88db2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4242c36-4936-45c9-a8e7-33ff9a312586",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': \"binary_logloss\",\n",
    "    'boosting': 'dart',\n",
    "    'device': \"cpu\",\n",
    "    'seed': seed,\n",
    "    'num_leaves': 85,\n",
    "    'learning_rate': 0.011,\n",
    "    'feature_fraction': 0.195,\n",
    "    'bagging_freq': 8,\n",
    "    'bagging_fraction': 0.55,\n",
    "    'n_jobs': -1,\n",
    "    'lambda_l2': 15,\n",
    "    'min_data_in_leaf': 75,\n",
    "    'scale_pos_weight': 1.28,\n",
    "    'max_bins': 255\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c57a3a3d-a51c-436e-9b0b-d19a66668b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29b170e8-eac9-4d09-970d-14244f215eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_est = [9000, 9000, 6500, 7500, 7000]\n",
    "n_est = [8500] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "789eb69b-371a-412f-b6d8-63b7432e0382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 4014 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 3\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.193062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 513689\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 4000\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.303624\ttraining's amex: 0.773844\tvalid_1's binary_logloss: 0.30703\tvalid_1's amex: 0.765619\n",
      "[1000]\ttraining's binary_logloss: 0.249238\ttraining's amex: 0.788008\tvalid_1's binary_logloss: 0.255561\tvalid_1's amex: 0.776167\n",
      "[1500]\ttraining's binary_logloss: 0.224394\ttraining's amex: 0.801604\tvalid_1's binary_logloss: 0.234497\tvalid_1's amex: 0.782982\n",
      "[2000]\ttraining's binary_logloss: 0.211537\ttraining's amex: 0.813905\tvalid_1's binary_logloss: 0.226196\tvalid_1's amex: 0.78828\n",
      "[2500]\ttraining's binary_logloss: 0.204751\ttraining's amex: 0.823815\tvalid_1's binary_logloss: 0.223259\tvalid_1's amex: 0.790743\n",
      "[3000]\ttraining's binary_logloss: 0.198434\ttraining's amex: 0.832861\tvalid_1's binary_logloss: 0.221309\tvalid_1's amex: 0.793598\n",
      "[3500]\ttraining's binary_logloss: 0.192771\ttraining's amex: 0.841662\tvalid_1's binary_logloss: 0.219954\tvalid_1's amex: 0.794397\n",
      "[4000]\ttraining's binary_logloss: 0.187251\ttraining's amex: 0.850208\tvalid_1's binary_logloss: 0.218985\tvalid_1's amex: 0.795673\n",
      "[4500]\ttraining's binary_logloss: 0.181953\ttraining's amex: 0.858755\tvalid_1's binary_logloss: 0.218286\tvalid_1's amex: 0.796875\n",
      "[5000]\ttraining's binary_logloss: 0.177185\ttraining's amex: 0.867387\tvalid_1's binary_logloss: 0.217802\tvalid_1's amex: 0.797129\n",
      "[5500]\ttraining's binary_logloss: 0.172998\ttraining's amex: 0.874927\tvalid_1's binary_logloss: 0.21751\tvalid_1's amex: 0.797184\n",
      "[6000]\ttraining's binary_logloss: 0.168788\ttraining's amex: 0.882578\tvalid_1's binary_logloss: 0.217279\tvalid_1's amex: 0.796809\n",
      "[6500]\ttraining's binary_logloss: 0.16425\ttraining's amex: 0.890173\tvalid_1's binary_logloss: 0.217011\tvalid_1's amex: 0.79686\n",
      "[7000]\ttraining's binary_logloss: 0.160299\ttraining's amex: 0.897405\tvalid_1's binary_logloss: 0.21687\tvalid_1's amex: 0.797348\n",
      "[7500]\ttraining's binary_logloss: 0.156096\ttraining's amex: 0.904359\tvalid_1's binary_logloss: 0.216678\tvalid_1's amex: 0.797332\n",
      "[8000]\ttraining's binary_logloss: 0.152385\ttraining's amex: 0.911219\tvalid_1's binary_logloss: 0.216563\tvalid_1's amex: 0.797173\n",
      "[8500]\ttraining's binary_logloss: 0.148671\ttraining's amex: 0.917585\tvalid_1's binary_logloss: 0.21641\tvalid_1's amex: 0.797221\n",
      "Our fold 3 CV score is 0.7970733567333925\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 4 with 4014 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 4\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95063, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 3.105567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 513769\n",
      "[LightGBM] [Info] Number of data points in the train set: 367131, number of used features: 4001\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258935 -> initscore=-1.051512\n",
      "[LightGBM] [Info] Start training from score -1.051512\n",
      "[500]\ttraining's binary_logloss: 0.303621\ttraining's amex: 0.774561\tvalid_1's binary_logloss: 0.306942\tvalid_1's amex: 0.763865\n",
      "[1000]\ttraining's binary_logloss: 0.249207\ttraining's amex: 0.788933\tvalid_1's binary_logloss: 0.255147\tvalid_1's amex: 0.773773\n",
      "[1500]\ttraining's binary_logloss: 0.224419\ttraining's amex: 0.801579\tvalid_1's binary_logloss: 0.233999\tvalid_1's amex: 0.782313\n",
      "[2000]\ttraining's binary_logloss: 0.211612\ttraining's amex: 0.813806\tvalid_1's binary_logloss: 0.225641\tvalid_1's amex: 0.787568\n",
      "[2500]\ttraining's binary_logloss: 0.204773\ttraining's amex: 0.823555\tvalid_1's binary_logloss: 0.222647\tvalid_1's amex: 0.790174\n",
      "[3000]\ttraining's binary_logloss: 0.19847\ttraining's amex: 0.83273\tvalid_1's binary_logloss: 0.220685\tvalid_1's amex: 0.792331\n",
      "[3500]\ttraining's binary_logloss: 0.192803\ttraining's amex: 0.840996\tvalid_1's binary_logloss: 0.219288\tvalid_1's amex: 0.793015\n",
      "[4000]\ttraining's binary_logloss: 0.187259\ttraining's amex: 0.84974\tvalid_1's binary_logloss: 0.218244\tvalid_1's amex: 0.794055\n",
      "[4500]\ttraining's binary_logloss: 0.181971\ttraining's amex: 0.858676\tvalid_1's binary_logloss: 0.217682\tvalid_1's amex: 0.794782\n",
      "[5000]\ttraining's binary_logloss: 0.177151\ttraining's amex: 0.867078\tvalid_1's binary_logloss: 0.217253\tvalid_1's amex: 0.795008\n",
      "[5500]\ttraining's binary_logloss: 0.172942\ttraining's amex: 0.874905\tvalid_1's binary_logloss: 0.217008\tvalid_1's amex: 0.79476\n",
      "[6000]\ttraining's binary_logloss: 0.168718\ttraining's amex: 0.882757\tvalid_1's binary_logloss: 0.216838\tvalid_1's amex: 0.794677\n",
      "[6500]\ttraining's binary_logloss: 0.164182\ttraining's amex: 0.890043\tvalid_1's binary_logloss: 0.216617\tvalid_1's amex: 0.795642\n",
      "[7000]\ttraining's binary_logloss: 0.160218\ttraining's amex: 0.897528\tvalid_1's binary_logloss: 0.216494\tvalid_1's amex: 0.795563\n",
      "[7500]\ttraining's binary_logloss: 0.156039\ttraining's amex: 0.904606\tvalid_1's binary_logloss: 0.21635\tvalid_1's amex: 0.795454\n",
      "[8000]\ttraining's binary_logloss: 0.152286\ttraining's amex: 0.911446\tvalid_1's binary_logloss: 0.216268\tvalid_1's amex: 0.794677\n",
      "[8500]\ttraining's binary_logloss: 0.148576\ttraining's amex: 0.917828\tvalid_1's binary_logloss: 0.216198\tvalid_1's amex: 0.795186\n",
      "Our fold 4 CV score is 0.7949749786056541\n"
     ]
    }
   ],
   "source": [
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_agg, target)):\n",
    "    if fold <= 2:\n",
    "        continue\n",
    "    n_estimator = n_est[fold]\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {train_agg.shape[1]} features...')\n",
    "    print('-'*50)\n",
    "    x_train, x_val = train_agg.iloc[trn_ind], train_agg.iloc[val_ind]\n",
    "    y_train, y_val = target[trn_ind], target[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "    print(f\"Start Training fold {fold}\")\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = n_estimator,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 300,\n",
    "        verbose_eval = 500,\n",
    "        feval = lgb_amex_metric\n",
    "    )\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'./models/model_fold{fold}_seed{seed}.pkl')\n",
    "    # Predict validation\n",
    "    y_val_pred = model.predict(x_val, raw_score=True)\n",
    "    val_score, val_g, val_t4 = amex_metric(y_val, y_val_pred)                                      \n",
    "    print(f'Our fold {fold} CV score is {val_score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3491b4e8-78bc-4f47-bb8d-2fac68019bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "Training fold 0 with 4014 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 0\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.367277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 513544\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 4001\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.303691\ttraining's amex: 0.774226\tvalid_1's binary_logloss: 0.30712\tvalid_1's amex: 0.766694\n",
      "[1000]\ttraining's binary_logloss: 0.249276\ttraining's amex: 0.78819\tvalid_1's binary_logloss: 0.255459\tvalid_1's amex: 0.776585\n",
      "[1500]\ttraining's binary_logloss: 0.22447\ttraining's amex: 0.800549\tvalid_1's binary_logloss: 0.234297\tvalid_1's amex: 0.784512\n",
      "[2000]\ttraining's binary_logloss: 0.211644\ttraining's amex: 0.813778\tvalid_1's binary_logloss: 0.22595\tvalid_1's amex: 0.789315\n",
      "[2500]\ttraining's binary_logloss: 0.20491\ttraining's amex: 0.822543\tvalid_1's binary_logloss: 0.223035\tvalid_1's amex: 0.790881\n",
      "[3000]\ttraining's binary_logloss: 0.198579\ttraining's amex: 0.831801\tvalid_1's binary_logloss: 0.220947\tvalid_1's amex: 0.793455\n",
      "[3500]\ttraining's binary_logloss: 0.192927\ttraining's amex: 0.841385\tvalid_1's binary_logloss: 0.219489\tvalid_1's amex: 0.795336\n",
      "[4000]\ttraining's binary_logloss: 0.187445\ttraining's amex: 0.849908\tvalid_1's binary_logloss: 0.218464\tvalid_1's amex: 0.796135\n",
      "[4500]\ttraining's binary_logloss: 0.182164\ttraining's amex: 0.859205\tvalid_1's binary_logloss: 0.217786\tvalid_1's amex: 0.796788\n",
      "[5000]\ttraining's binary_logloss: 0.17739\ttraining's amex: 0.867499\tvalid_1's binary_logloss: 0.217344\tvalid_1's amex: 0.796496\n",
      "[5500]\ttraining's binary_logloss: 0.173207\ttraining's amex: 0.875114\tvalid_1's binary_logloss: 0.217058\tvalid_1's amex: 0.796123\n",
      "[6000]\ttraining's binary_logloss: 0.168966\ttraining's amex: 0.882206\tvalid_1's binary_logloss: 0.216742\tvalid_1's amex: 0.796053\n",
      "[6500]\ttraining's binary_logloss: 0.164411\ttraining's amex: 0.889757\tvalid_1's binary_logloss: 0.21643\tvalid_1's amex: 0.796726\n",
      "[7000]\ttraining's binary_logloss: 0.160476\ttraining's amex: 0.897177\tvalid_1's binary_logloss: 0.216252\tvalid_1's amex: 0.796902\n",
      "[7500]\ttraining's binary_logloss: 0.156262\ttraining's amex: 0.903962\tvalid_1's binary_logloss: 0.216014\tvalid_1's amex: 0.797431\n",
      "[8000]\ttraining's binary_logloss: 0.152556\ttraining's amex: 0.910746\tvalid_1's binary_logloss: 0.215872\tvalid_1's amex: 0.797208\n",
      "[8500]\ttraining's binary_logloss: 0.148858\ttraining's amex: 0.917045\tvalid_1's binary_logloss: 0.215778\tvalid_1's amex: 0.797223\n",
      "Our fold 0 CV score is 0.7969844492536128\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 1 with 4014 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 1\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.176714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 513663\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 4001\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.303654\ttraining's amex: 0.773687\tvalid_1's binary_logloss: 0.307165\tvalid_1's amex: 0.765902\n",
      "[1000]\ttraining's binary_logloss: 0.249139\ttraining's amex: 0.789051\tvalid_1's binary_logloss: 0.255579\tvalid_1's amex: 0.774305\n",
      "[1500]\ttraining's binary_logloss: 0.224402\ttraining's amex: 0.802149\tvalid_1's binary_logloss: 0.234488\tvalid_1's amex: 0.78152\n",
      "[2000]\ttraining's binary_logloss: 0.211609\ttraining's amex: 0.814329\tvalid_1's binary_logloss: 0.226087\tvalid_1's amex: 0.786621\n",
      "[2500]\ttraining's binary_logloss: 0.204851\ttraining's amex: 0.823539\tvalid_1's binary_logloss: 0.223035\tvalid_1's amex: 0.788933\n",
      "[3000]\ttraining's binary_logloss: 0.198525\ttraining's amex: 0.832712\tvalid_1's binary_logloss: 0.220957\tvalid_1's amex: 0.791372\n",
      "[3500]\ttraining's binary_logloss: 0.192889\ttraining's amex: 0.841408\tvalid_1's binary_logloss: 0.219594\tvalid_1's amex: 0.792495\n",
      "[4000]\ttraining's binary_logloss: 0.187387\ttraining's amex: 0.850088\tvalid_1's binary_logloss: 0.218589\tvalid_1's amex: 0.79384\n",
      "[4500]\ttraining's binary_logloss: 0.182106\ttraining's amex: 0.858979\tvalid_1's binary_logloss: 0.217791\tvalid_1's amex: 0.794593\n",
      "[5000]\ttraining's binary_logloss: 0.177332\ttraining's amex: 0.867465\tvalid_1's binary_logloss: 0.217347\tvalid_1's amex: 0.794626\n",
      "[5500]\ttraining's binary_logloss: 0.173078\ttraining's amex: 0.874796\tvalid_1's binary_logloss: 0.217019\tvalid_1's amex: 0.795198\n",
      "[6000]\ttraining's binary_logloss: 0.168884\ttraining's amex: 0.882651\tvalid_1's binary_logloss: 0.216773\tvalid_1's amex: 0.79635\n",
      "[6500]\ttraining's binary_logloss: 0.164364\ttraining's amex: 0.890011\tvalid_1's binary_logloss: 0.216524\tvalid_1's amex: 0.795659\n",
      "[7000]\ttraining's binary_logloss: 0.160439\ttraining's amex: 0.897325\tvalid_1's binary_logloss: 0.21633\tvalid_1's amex: 0.795606\n",
      "[7500]\ttraining's binary_logloss: 0.156232\ttraining's amex: 0.904444\tvalid_1's binary_logloss: 0.216128\tvalid_1's amex: 0.79568\n",
      "[8000]\ttraining's binary_logloss: 0.1525\ttraining's amex: 0.910801\tvalid_1's binary_logloss: 0.216062\tvalid_1's amex: 0.795665\n",
      "[8500]\ttraining's binary_logloss: 0.14878\ttraining's amex: 0.91694\tvalid_1's binary_logloss: 0.215969\tvalid_1's amex: 0.79592\n",
      "Our fold 1 CV score is 0.7956804948702045\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 2 with 4014 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 2\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n",
      "[LightGBM] [Info] Number of positive: 95062, number of negative: 272068\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 1.115992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 513437\n",
      "[LightGBM] [Info] Number of data points in the train set: 367130, number of used features: 4002\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258933 -> initscore=-1.051523\n",
      "[LightGBM] [Info] Start training from score -1.051523\n",
      "[500]\ttraining's binary_logloss: 0.303094\ttraining's amex: 0.775741\tvalid_1's binary_logloss: 0.308711\tvalid_1's amex: 0.762154\n",
      "[1000]\ttraining's binary_logloss: 0.248673\ttraining's amex: 0.790078\tvalid_1's binary_logloss: 0.257371\tvalid_1's amex: 0.770173\n",
      "[1500]\ttraining's binary_logloss: 0.223781\ttraining's amex: 0.802988\tvalid_1's binary_logloss: 0.236209\tvalid_1's amex: 0.779523\n",
      "[2000]\ttraining's binary_logloss: 0.211007\ttraining's amex: 0.81544\tvalid_1's binary_logloss: 0.227836\tvalid_1's amex: 0.785438\n",
      "[2500]\ttraining's binary_logloss: 0.204264\ttraining's amex: 0.824184\tvalid_1's binary_logloss: 0.224893\tvalid_1's amex: 0.7871\n",
      "[3000]\ttraining's binary_logloss: 0.197915\ttraining's amex: 0.833359\tvalid_1's binary_logloss: 0.222842\tvalid_1's amex: 0.788452\n",
      "[3500]\ttraining's binary_logloss: 0.192289\ttraining's amex: 0.842202\tvalid_1's binary_logloss: 0.221499\tvalid_1's amex: 0.790215\n",
      "[4000]\ttraining's binary_logloss: 0.186798\ttraining's amex: 0.85074\tvalid_1's binary_logloss: 0.220516\tvalid_1's amex: 0.791074\n",
      "[4500]\ttraining's binary_logloss: 0.181513\ttraining's amex: 0.860043\tvalid_1's binary_logloss: 0.219807\tvalid_1's amex: 0.791911\n",
      "[5000]\ttraining's binary_logloss: 0.176735\ttraining's amex: 0.868333\tvalid_1's binary_logloss: 0.219356\tvalid_1's amex: 0.792205\n",
      "[5500]\ttraining's binary_logloss: 0.172576\ttraining's amex: 0.875668\tvalid_1's binary_logloss: 0.219005\tvalid_1's amex: 0.79321\n",
      "[6000]\ttraining's binary_logloss: 0.168387\ttraining's amex: 0.883318\tvalid_1's binary_logloss: 0.218699\tvalid_1's amex: 0.79346\n",
      "[6500]\ttraining's binary_logloss: 0.163867\ttraining's amex: 0.890897\tvalid_1's binary_logloss: 0.218442\tvalid_1's amex: 0.792567\n",
      "[7000]\ttraining's binary_logloss: 0.15989\ttraining's amex: 0.897825\tvalid_1's binary_logloss: 0.218242\tvalid_1's amex: 0.792807\n",
      "[7500]\ttraining's binary_logloss: 0.155699\ttraining's amex: 0.904837\tvalid_1's binary_logloss: 0.218047\tvalid_1's amex: 0.79323\n",
      "[8000]\ttraining's binary_logloss: 0.15198\ttraining's amex: 0.911489\tvalid_1's binary_logloss: 0.217971\tvalid_1's amex: 0.79301\n",
      "[8500]\ttraining's binary_logloss: 0.148275\ttraining's amex: 0.917853\tvalid_1's binary_logloss: 0.217913\tvalid_1's amex: 0.792506\n",
      "Our fold 2 CV score is 0.792267110942235\n",
      " \n",
      "--------------------------------------------------\n",
      "Training fold 3 with 4014 features...\n",
      "--------------------------------------------------\n",
      "Start Training fold 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m lgb_valid \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mDataset(x_val, y_val, categorical_feature\u001b[38;5;241m=\u001b[39mcat_features)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart Training fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlgb_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlgb_valid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlgb_amex_metric\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Save best model\u001b[39;00m\n\u001b[1;32m     22\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(model, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models/model_fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_seed\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/amex/lib/python3.10/site-packages/lightgbm/engine.py:271\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 271\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[1;32m    273\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/amex/lib/python3.10/site-packages/lightgbm/basic.py:2605\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[1;32m   2598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_network(\n\u001b[1;32m   2599\u001b[0m         machines\u001b[38;5;241m=\u001b[39mmachines,\n\u001b[1;32m   2600\u001b[0m         local_listen_port\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_listen_port\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   2601\u001b[0m         listen_time_out\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m120\u001b[39m),\n\u001b[1;32m   2602\u001b[0m         num_machines\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_machines\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   2603\u001b[0m     )\n\u001b[1;32m   2604\u001b[0m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[0;32m-> 2605\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[1;32m   2607\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n",
      "File \u001b[0;32m~/miniconda3/envs/amex/lib/python3.10/site-packages/lightgbm/basic.py:1815\u001b[0m, in \u001b[0;36mDataset.construct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1812\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_init_score_by_predictor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, used_indices)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1814\u001b[0m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[0;32m-> 1815\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1816\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1817\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1819\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_raw_data:\n\u001b[1;32m   1821\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/amex/lib/python3.10/site-packages/lightgbm/basic.py:1538\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__init_from_csc(data, params_str, ref_dataset)\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m-> 1538\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__init_from_np2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):\n",
      "File \u001b[0;32m~/miniconda3/envs/amex/lib/python3.10/site-packages/lightgbm/basic.py:1654\u001b[0m, in \u001b[0;36mDataset.__init_from_np2d\u001b[0;34m(self, mat, params_str, ref_dataset)\u001b[0m\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_void_p()\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mat\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32 \u001b[38;5;129;01mor\u001b[39;00m mat\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64:\n\u001b[0;32m-> 1654\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mmat\u001b[38;5;241m.\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1655\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# change non-float data to float data, need to copy\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(mat\u001b[38;5;241m.\u001b[39mreshape(mat\u001b[38;5;241m.\u001b[39msize), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold, (trn_ind, val_ind) in enumerate(kfold.split(train_agg, target)):\n",
    "    n_estimator = n_est[fold]\n",
    "    print(' ')\n",
    "    print('-'*50)\n",
    "    print(f'Training fold {fold} with {train_agg.shape[1]} features...')\n",
    "    print('-'*50)\n",
    "    x_train, x_val = train_agg.iloc[trn_ind], train_agg.iloc[val_ind]\n",
    "    y_train, y_val = target[trn_ind], target[val_ind]\n",
    "    lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=cat_features)\n",
    "    lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature=cat_features)\n",
    "    print(f\"Start Training fold {fold}\")\n",
    "    model = lgb.train(\n",
    "        params = params,\n",
    "        train_set = lgb_train,\n",
    "        num_boost_round = n_estimator,\n",
    "        valid_sets = [lgb_train, lgb_valid],\n",
    "        early_stopping_rounds = 300,\n",
    "        verbose_eval = 500,\n",
    "        feval = lgb_amex_metric\n",
    "    )\n",
    "    # Save best model\n",
    "    joblib.dump(model, f'./models/model_fold{fold}_seed{seed}.pkl')\n",
    "    # Predict validation\n",
    "    y_val_pred = model.predict(x_val, raw_score=True)\n",
    "    val_score, val_g, val_t4 = amex_metric(y_val, y_val_pred)                                      \n",
    "    print(f'Our fold {fold} CV score is {val_score}')\n",
    "    del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04680e2b-df75-44f4-a2f1-6c2c0f90f004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex",
   "language": "python",
   "name": "amex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
